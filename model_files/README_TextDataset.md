# ğŸ“š TextDataset é¡è©³è§£

`TextDataset` æ˜¯ä¸€å€‹è‡ªå®šç¾©çš„ PyTorch æ•¸æ“šé›†é¡ï¼Œå°ˆé–€ç”¨æ–¼è™•ç†æ–‡æœ¬æ•¸æ“šä¸¦ç‚º Transformer æ¨¡å‹è¨“ç·´åšæº–å‚™ã€‚

## ğŸ—ï¸ é¡çµæ§‹æ¦‚è¦½

```python
class TextDataset(Dataset):
    """æ–‡æœ¬æ•¸æ“šé›†é¡"""
    def __init__(self, text_file, vocab_size=10000, max_seq_len=128)
    def preprocess_text(self, text)
    def build_vocab(self, vocab_size) 
    def tokenize_sentences(self)
    def __len__(self)
    def __getitem__(self, idx)
```

## ğŸ”§ è©³ç´°åŠŸèƒ½åˆ†æ

### 1. **åˆå§‹åŒ–æ–¹æ³• `__init__`**

```python
def __init__(self, text_file, vocab_size=10000, max_seq_len=128):
    self.max_seq_len = max_seq_len
    
    # è®€å–æ–‡æœ¬æ•¸æ“š
    with open(text_file, 'r', encoding='utf-8') as f:
        text = f.read()
    
    # æ¸…ç†å’Œé è™•ç†æ–‡æœ¬
    self.sentences = self.preprocess_text(text)
    
    # æ§‹å»ºè©å½™è¡¨
    self.build_vocab(vocab_size)
    
    # å°‡å¥å­è½‰æ›ç‚ºtokenåºåˆ—
    self.tokenized_sentences = self.tokenize_sentences()
```

**ä½œç”¨**ï¼š
- ğŸ“ è®€å–æ–‡æœ¬æ–‡ä»¶
- ğŸ§¹ æ¸…ç†å’Œé è™•ç†æ–‡æœ¬
- ğŸ“– æ§‹å»ºè©å½™è¡¨
- ğŸ”¢ å°‡æ–‡æœ¬è½‰æ›ç‚ºæ•¸å­—åºåˆ—

**åƒæ•¸èªªæ˜**ï¼š
- `text_file`: è¼¸å…¥æ–‡æœ¬æ–‡ä»¶è·¯å¾‘
- `vocab_size`: è©å½™è¡¨å¤§å°ï¼ˆé»˜èª10,000ï¼‰
- `max_seq_len`: æœ€å¤§åºåˆ—é•·åº¦ï¼ˆé»˜èª128ï¼‰

### 2. **æ–‡æœ¬é è™•ç† `preprocess_text`**

```python
def preprocess_text(self, text):
    """é è™•ç†æ–‡æœ¬ï¼Œåˆ†å‰²æˆå¥å­"""
    # ç§»é™¤å¤šé¤˜ç©ºç™½å’Œæ›è¡Œ
    text = re.sub(r'\n+', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    
    # ç°¡å–®çš„å¥å­åˆ†å‰²ï¼ˆä»¥å¥è™Ÿã€å•è™Ÿã€é©šå˜†è™Ÿåˆ†å‰²ï¼‰
    sentences = re.split(r'[.!?]+', text)
    
    # æ¸…ç†æ¯å€‹å¥å­
    cleaned_sentences = []
    for sent in sentences:
        sent = sent.strip()
        if len(sent) > 10:  # éæ¿¾å¤ªçŸ­çš„å¥å­
            # ä¿ç•™å­—æ¯ã€æ•¸å­—ã€ç©ºæ ¼å’ŒåŸºæœ¬æ¨™é»
            sent = re.sub(r'[^a-zA-Z0-9\s\',;:-]', '', sent)
            sent = re.sub(r'\s+', ' ', sent).strip()
            if sent:
                cleaned_sentences.append(sent.lower())
    
    return cleaned_sentences[:5000]  # é™åˆ¶æ•¸æ“šé‡
```

**è™•ç†æ­¥é©Ÿ**ï¼š
1. ğŸ”„ **æ¨™æº–åŒ–ç©ºç™½**ï¼šå°‡å¤šå€‹æ›è¡Œç¬¦å’Œç©ºæ ¼åˆä½µ
2. âœ‚ï¸ **å¥å­åˆ†å‰²**ï¼šæ ¹æ“šæ¨™é»ç¬¦è™Ÿ `.!?` åˆ†å‰²å¥å­
3. ğŸ§¼ **æ¸…ç†æ–‡æœ¬**ï¼šç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™åŸºæœ¬æ¨™é»
4. ğŸ“ **éæ¿¾çŸ­å¥**ï¼šç§»é™¤å¤ªçŸ­çš„å¥å­ï¼ˆå°‘æ–¼10å€‹å­—ç¬¦ï¼‰
5. ğŸ”¤ **è½‰å°å¯«**ï¼šçµ±ä¸€æ–‡æœ¬æ ¼å¼
6. ğŸ“Š **é™åˆ¶æ•¸é‡**ï¼šåªå–å‰5000å€‹å¥å­ï¼ˆåŠ å¿«è¨“ç·´ï¼‰

**ç¤ºä¾‹è½‰æ›**ï¼š
```
åŸå§‹æ–‡æœ¬:
"First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak."

é è™•ç†å¾Œ:
["before we proceed any further, hear me speak", "speak, speak"]
```

### 3. **è©å½™è¡¨æ§‹å»º `build_vocab`**

```python
def build_vocab(self, vocab_size):
    """æ§‹å»ºè©å½™è¡¨"""
    # ç‰¹æ®Štoken
    self.pad_idx = 0  # å¡«å……token
    self.unk_idx = 1  # æœªçŸ¥token
    self.bos_idx = 2  # å¥å­é–‹å§‹token
    self.eos_idx = 3  # å¥å­çµæŸtoken
    
    special_tokens = ['<PAD>', '<UNK>', '<BOS>', '<EOS>']
    
    # çµ±è¨ˆè©é »
    word_counts = Counter()
    for sentence in self.sentences:
        words = sentence.split()
        word_counts.update(words)
    
    # é¸æ“‡æœ€å¸¸è¦‹çš„è©
    most_common = word_counts.most_common(vocab_size - len(special_tokens))
    
    # æ§‹å»ºè©å½™è¡¨
    vocab_list = special_tokens + [word for word, _ in most_common]
    self.vocab = {word: idx for idx, word in enumerate(vocab_list)}
    self.idx_to_vocab = {idx: word for word, idx in self.vocab.items()}
```

**è©å½™è¡¨çµæ§‹**ï¼š

| ç´¢å¼• | Token  | èªªæ˜           |
|------|--------|----------------|
| 0    | `<PAD>` | å¡«å……ï¼ˆåºåˆ—å°é½Šï¼‰ |
| 1    | `<UNK>` | æœªçŸ¥è©         |
| 2    | `<BOS>` | å¥å­é–‹å§‹       |
| 3    | `<EOS>` | å¥å­çµæŸ       |
| 4    | the    | æœ€å¸¸è¦‹è©1      |
| 5    | and    | æœ€å¸¸è¦‹è©2      |
| ...  | ...    | ...           |

**ç‰¹æ®Š Token çš„ä½œç”¨**ï¼š
- **`<PAD>`**ï¼šå¡«å……è¼ƒçŸ­çš„åºåˆ—ï¼Œä½¿æ‰¹æ¬¡ä¸­æ‰€æœ‰åºåˆ—é•·åº¦ç›¸åŒ
- **`<UNK>`**ï¼šè¡¨ç¤ºè©å½™è¡¨ä¸­ä¸å­˜åœ¨çš„è©
- **`<BOS>`**ï¼šæ¨™è¨˜å¥å­é–‹å§‹ï¼Œå¹«åŠ©æ¨¡å‹ç†è§£åºåˆ—èµ·é»
- **`<EOS>`**ï¼šæ¨™è¨˜å¥å­çµæŸï¼Œå¹«åŠ©æ¨¡å‹çŸ¥é“ä½•æ™‚åœæ­¢ç”Ÿæˆ

### 4. **TokenåŒ– `tokenize_sentences`**

```python
def tokenize_sentences(self):
    """å°‡å¥å­è½‰æ›ç‚ºtokenåºåˆ—"""
    tokenized = []
    for sentence in self.sentences:
        words = sentence.split()
        # æ·»åŠ BOSå’ŒEOSæ¨™è¨˜
        tokens = [self.bos_idx] + [
            self.vocab.get(word, self.unk_idx) for word in words
        ] + [self.eos_idx]
        
        # æˆªæ–·æˆ–å¡«å……åˆ°å›ºå®šé•·åº¦
        if len(tokens) > self.max_seq_len:
            tokens = tokens[:self.max_seq_len]
        else:
            tokens += [self.pad_idx] * (self.max_seq_len - len(tokens))
        
        tokenized.append(tokens)
    
    return tokenized
```

**è½‰æ›ç¤ºä¾‹**ï¼š

```
æ­¥é©Ÿ1 - åŸå¥: "to be or not to be"
       â†“
æ­¥é©Ÿ2 - è©åˆ†å‰²: ["to", "be", "or", "not", "to", "be"]
       â†“
æ­¥é©Ÿ3 - æ·»åŠ ç‰¹æ®Štoken: [<BOS>, "to", "be", "or", "not", "to", "be", <EOS>]
       â†“
æ­¥é©Ÿ4 - è½‰æ›ç‚ºç´¢å¼•: [2, 45, 67, 23, 89, 45, 67, 3]
       â†“
æ­¥é©Ÿ5 - å¡«å……åˆ°å›ºå®šé•·åº¦ (å‡è¨­max_seq_len=12): 
        [2, 45, 67, 23, 89, 45, 67, 3, 0, 0, 0, 0]
```

### 5. **æ•¸æ“šé›†æ¥å£æ–¹æ³•**

```python
def __len__(self):
    return len(self.tokenized_sentences)

def __getitem__(self, idx):
    tokens = self.tokenized_sentences[idx]
    
    # å°æ–¼ Transformer è¨“ç·´ï¼Œå‰µå»ºæºåºåˆ—å’Œç›®æ¨™åºåˆ—
    src = torch.tensor(tokens, dtype=torch.long)
    tgt_input = torch.tensor(tokens[:-1], dtype=torch.long)   # ä¸åŒ…å«æœ€å¾Œä¸€å€‹token
    tgt_output = torch.tensor(tokens[1:], dtype=torch.long)   # ä¸åŒ…å«ç¬¬ä¸€å€‹token
    
    return src, tgt_input, tgt_output
```

**æ•¸æ“šæ ¼å¼èªªæ˜**ï¼š

```
åŸåºåˆ—:    [<BOS>, w1, w2, w3, <EOS>, <PAD>, <PAD>, ...]
           
è¿”å›çš„ä¸‰å€‹å¼µé‡:
src:       [<BOS>, w1, w2, w3, <EOS>, <PAD>, <PAD>, ...]  # ç·¨ç¢¼å™¨è¼¸å…¥ï¼ˆå®Œæ•´åºåˆ—ï¼‰
tgt_input: [<BOS>, w1, w2, w3, <EOS>, <PAD>, ...]        # è§£ç¢¼å™¨è¼¸å…¥ï¼ˆå»æ‰æœ€å¾Œä¸€å€‹ï¼‰
tgt_output:[w1, w2, w3, <EOS>, <PAD>, <PAD>, ...]        # è§£ç¢¼å™¨ç›®æ¨™ï¼ˆå»æ‰ç¬¬ä¸€å€‹ï¼‰
```

## ğŸ¯ ç‚ºä»€éº¼é€™æ¨£è¨­è¨ˆï¼Ÿ

### 1. **Teacher Forcing è¨“ç·´ç­–ç•¥**

Teacher Forcing æ˜¯è¨“ç·´åºåˆ—ç”Ÿæˆæ¨¡å‹çš„æ¨™æº–æŠ€è¡“ï¼š

- åœ¨è¨“ç·´éšæ®µï¼Œè§£ç¢¼å™¨èƒ½çœ‹åˆ°æ­£ç¢ºçš„å‰ä¸€å€‹ token
- `tgt_input` å’Œ `tgt_output` éŒ¯ä½ä¸€å€‹ä½ç½®
- é€™åŠ å¿«äº†è¨“ç·´æ”¶æ–‚é€Ÿåº¦

**ç¤ºä¾‹**ï¼š
```
è¦å­¸ç¿’ç”Ÿæˆ: "hello world"

tgt_input:  [<BOS>, hello]      # è§£ç¢¼å™¨è¼¸å…¥
tgt_output: [hello, world]      # æœŸæœ›è¼¸å‡º

æ¨¡å‹å­¸ç¿’: çµ¦å®š <BOS> è¼¸å‡º "hello"ï¼Œçµ¦å®š "hello" è¼¸å‡º "world"
```

### 2. **å›ºå®šåºåˆ—é•·åº¦çš„å¿…è¦æ€§**

```python
# æ‰¹æ¬¡è™•ç†éœ€è¦ç›¸åŒå½¢ç‹€çš„å¼µé‡
batch = [
    [2, 45, 67, 3, 0, 0],    # å¥å­1ï¼ˆå¡«å……äº†2å€‹<PAD>ï¼‰
    [2, 12, 34, 56, 78, 3],  # å¥å­2ï¼ˆæ°å¥½å¡«æ»¿ï¼‰
    [2, 23, 45, 3, 0, 0],    # å¥å­3ï¼ˆå¡«å……äº†2å€‹<PAD>ï¼‰
]
# æ‰€æœ‰åºåˆ—é•·åº¦ç›¸åŒï¼Œå¯ä»¥çµ„æˆæ‰¹æ¬¡é€²è¡Œä¸¦è¡Œè™•ç†
```

### 3. **Transformer æ¶æ§‹é©é…**

Encoder-Decoder Transformer éœ€è¦ï¼š
- **ç·¨ç¢¼å™¨è¼¸å…¥** (`src`)ï¼šå®Œæ•´çš„æºåºåˆ—
- **è§£ç¢¼å™¨è¼¸å…¥** (`tgt_input`)ï¼šç›®æ¨™åºåˆ—ï¼ˆç”¨æ–¼ teacher forcingï¼‰
- **è§£ç¢¼å™¨ç›®æ¨™** (`tgt_output`)ï¼šæœŸæœ›è¼¸å‡ºï¼ˆç”¨æ–¼è¨ˆç®—æå¤±ï¼‰

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

```python
# å‰µå»ºæ•¸æ“šé›†
dataset = TextDataset('data/input.txt', vocab_size=5000, max_seq_len=64)

# æŸ¥çœ‹æ•¸æ“šé›†ä¿¡æ¯
print(f"å¥å­æ•¸é‡: {len(dataset)}")
print(f"è©å½™è¡¨å¤§å°: {dataset.vocab_size}")
print(f"ç‰¹æ®Štokenç´¢å¼•: PAD={dataset.pad_idx}, UNK={dataset.unk_idx}")

# ç²å–ä¸€å€‹æ¨£æœ¬
src, tgt_input, tgt_output = dataset[0]
print(f"æºåºåˆ—å½¢ç‹€: {src.shape}")           # torch.Size([64])
print(f"ç›®æ¨™è¼¸å…¥å½¢ç‹€: {tgt_input.shape}")    # torch.Size([63])
print(f"ç›®æ¨™è¼¸å‡ºå½¢ç‹€: {tgt_output.shape}")   # torch.Size([63])

# å‰µå»ºæ•¸æ“šåŠ è¼‰å™¨
from torch.utils.data import DataLoader
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# éæ­·æ‰¹æ¬¡
for batch_idx, (src_batch, tgt_input_batch, tgt_output_batch) in enumerate(dataloader):
    print(f"æ‰¹æ¬¡ {batch_idx}:")
    print(f"  æºåºåˆ—æ‰¹æ¬¡å½¢ç‹€: {src_batch.shape}")        # torch.Size([32, 64])
    print(f"  ç›®æ¨™è¼¸å…¥æ‰¹æ¬¡å½¢ç‹€: {tgt_input_batch.shape}")  # torch.Size([32, 63])
    print(f"  ç›®æ¨™è¼¸å‡ºæ‰¹æ¬¡å½¢ç‹€: {tgt_output_batch.shape}") # torch.Size([32, 63])
    break
```

## ğŸ” æ·±å…¥ç†è§£ï¼šå¾æ–‡æœ¬åˆ°æ¨¡å‹è¼¸å…¥

è®“æˆ‘å€‘è·Ÿè¹¤ä¸€å€‹å®Œæ•´çš„ä¾‹å­ï¼š

```python
# 1. åŸå§‹æ–‡æœ¬ï¼ˆinput.txt ä¸­çš„ä¸€æ®µï¼‰
raw_text = """
First Citizen:
Before we proceed any further, hear me speak.

All:
Speak, speak.
"""

# 2. é è™•ç†å¾Œçš„å¥å­
sentences = [
    "before we proceed any further, hear me speak",
    "speak, speak"
]

# 3. æ§‹å»ºè©å½™è¡¨ï¼ˆç°¡åŒ–ç‰ˆï¼‰
vocab = {
    '<PAD>': 0, '<UNK>': 1, '<BOS>': 2, '<EOS>': 3,
    'before': 4, 'we': 5, 'proceed': 6, 'any': 7, 
    'further': 8, 'hear': 9, 'me': 10, 'speak': 11
}

# 4. TokenåŒ–ç¬¬ä¸€å€‹å¥å­
sentence = "before we proceed any further, hear me speak"
words = ["before", "we", "proceed", "any", "further", "hear", "me", "speak"]
tokens = [2] + [vocab[word] for word in words] + [3]  # [2, 4, 5, 6, 7, 8, 9, 10, 11, 3]

# 5. å¡«å……åˆ°å›ºå®šé•·åº¦ï¼ˆå‡è¨­ max_seq_len=12ï¼‰
padded_tokens = [2, 4, 5, 6, 7, 8, 9, 10, 11, 3, 0, 0]

# 6. ç”Ÿæˆè¨“ç·´æ•¸æ“š
src = [2, 4, 5, 6, 7, 8, 9, 10, 11, 3, 0, 0]      # ç·¨ç¢¼å™¨è¼¸å…¥
tgt_input = [2, 4, 5, 6, 7, 8, 9, 10, 11, 3, 0]   # è§£ç¢¼å™¨è¼¸å…¥
tgt_output = [4, 5, 6, 7, 8, 9, 10, 11, 3, 0, 0]  # è§£ç¢¼å™¨ç›®æ¨™
```

## ğŸš€ æ€§èƒ½å„ªåŒ–å»ºè­°

1. **èª¿æ•´ `vocab_size`**ï¼š
   - å¤ªå°ï¼šå¾ˆå¤šè©è®Šæˆ `<UNK>`ï¼Œä¿¡æ¯ä¸Ÿå¤±
   - å¤ªå¤§ï¼šæ¨¡å‹åƒæ•¸å¢åŠ ï¼Œè¨“ç·´æ…¢

2. **èª¿æ•´ `max_seq_len`**ï¼š
   - å¤ªçŸ­ï¼šé•·å¥å­è¢«æˆªæ–·
   - å¤ªé•·ï¼šå…§å­˜ä½¿ç”¨å¢åŠ ï¼Œè¨“ç·´æ…¢

3. **æ•¸æ“šé è™•ç†å„ªåŒ–**ï¼š
   - å¯ä»¥æ·»åŠ æ›´è¤‡é›œçš„æ–‡æœ¬æ¸…ç†
   - è€ƒæ…®ä½¿ç”¨ BPE æˆ– SentencePiece tokenization

4. **æ‰¹æ¬¡å¤§å°èª¿æ•´**ï¼š
   ```python
   # æ ¹æ“šGPUå…§å­˜èª¿æ•´æ‰¹æ¬¡å¤§å°
   dataloader = DataLoader(dataset, batch_size=16, shuffle=True)
   ```

## ğŸ”§ è‡ªå®šç¾©æ“´å±•

æ‚¨å¯ä»¥æ ¹æ“šéœ€è¦æ“´å±• `TextDataset`ï¼š

```python
class AdvancedTextDataset(TextDataset):
    def __init__(self, text_file, vocab_size=10000, max_seq_len=128, 
                 use_bpe=False, min_freq=2):
        self.use_bpe = use_bpe
        self.min_freq = min_freq
        super().__init__(text_file, vocab_size, max_seq_len)
    
    def build_vocab(self, vocab_size):
        # å¯ä»¥æ·»åŠ æœ€å°è©é »éæ¿¾
        # å¯ä»¥æ•´åˆ BPE tokenization
        pass
    
    def preprocess_text(self, text):
        # å¯ä»¥æ·»åŠ æ›´è¤‡é›œçš„æ–‡æœ¬æ¸…ç†
        # ä¾‹å¦‚ï¼šè™•ç†ç‰¹æ®Šå­—ç¬¦ã€æ¨™æº–åŒ–ç­‰
        pass
```

é€™å€‹ `TextDataset` é¡æ˜¯æ•´å€‹ Transformer è¨“ç·´æµç¨‹çš„åŸºç¤ï¼Œå®ƒå°‡åŸå§‹æ–‡æœ¬è½‰æ›ç‚ºæ¨¡å‹å¯ä»¥ç†è§£å’Œè™•ç†çš„æ•¸å­—æ ¼å¼ï¼ğŸ“šğŸ¤– 