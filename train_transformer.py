import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import re
import pickle
import os
import getpass  # æ·»åŠ å®‰å…¨è¼¸å…¥æ¨¡çµ„
from collections import Counter
from tqdm import tqdm
import matplotlib.pyplot as plt
from encoder_decoder_transformer import Transformer

class TextDataset(Dataset):
    """æ–‡æœ¬æ•¸æ“šé›†é¡"""
    def __init__(self, text_file, vocab_size=10000, max_seq_len=128):
        self.max_seq_len = max_seq_len
        
        # è®€å–æ–‡æœ¬æ•¸æ“š
        with open(text_file, 'r', encoding='utf-8') as f:
            text = f.read()
        
        # æ¸…ç†å’Œé è™•ç†æ–‡æœ¬
        self.sentences = self.preprocess_text(text)
        
        # æ§‹å»ºè©å½™è¡¨
        self.build_vocab(vocab_size)
        
        # å°‡å¥å­è½‰æ›ç‚ºtokenåºåˆ—
        self.tokenized_sentences = self.tokenize_sentences()
        
        print(f"æ•¸æ“šé›†ä¿¡æ¯:")
        print(f"- ç¸½å¥å­æ•¸: {len(self.sentences)}")
        print(f"- è©å½™è¡¨å¤§å°: {len(self.vocab)}")
        print(f"- ç‰¹æ®Štoken: <PAD>={self.pad_idx}, <UNK>={self.unk_idx}, <BOS>={self.bos_idx}, <EOS>={self.eos_idx}")
    
    def preprocess_text(self, text):
        """é è™•ç†æ–‡æœ¬ï¼Œåˆ†å‰²æˆå¥å­"""
        # ç§»é™¤å¤šé¤˜ç©ºç™½å’Œæ›è¡Œ
        text = re.sub(r'\n+', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        # ç°¡å–®çš„å¥å­åˆ†å‰²ï¼ˆä»¥å¥è™Ÿã€å•è™Ÿã€é©šå˜†è™Ÿåˆ†å‰²ï¼‰
        sentences = re.split(r'[.!?]+', text)
        
        # æ¸…ç†æ¯å€‹å¥å­
        cleaned_sentences = []
        for sent in sentences:
            sent = sent.strip()
            if len(sent) > 10:  # éæ¿¾å¤ªçŸ­çš„å¥å­
                # ç°¡å–®çš„æ¸…ç†ï¼šä¿ç•™å­—æ¯ã€æ•¸å­—ã€ç©ºæ ¼å’ŒåŸºæœ¬æ¨™é»
                sent = re.sub(r'[^a-zA-Z0-9\s\',;:-]', '', sent)
                sent = re.sub(r'\s+', ' ', sent).strip()
                if sent:
                    cleaned_sentences.append(sent.lower())
        
        return cleaned_sentences[:5000]  # é™åˆ¶æ•¸æ“šé‡ä»¥åŠ å¿«è¨“ç·´
    
    def build_vocab(self, vocab_size):
        """æ§‹å»ºè©å½™è¡¨"""
        # ç‰¹æ®Štoken
        self.pad_idx = 0
        self.unk_idx = 1
        self.bos_idx = 2
        self.eos_idx = 3
        
        special_tokens = ['<PAD>', '<UNK>', '<BOS>', '<EOS>']
        
        # çµ±è¨ˆè©é »
        word_counts = Counter()
        for sentence in self.sentences:
            words = sentence.split()
            word_counts.update(words)
        
        # é¸æ“‡æœ€å¸¸è¦‹çš„è©
        most_common = word_counts.most_common(vocab_size - len(special_tokens))
        
        # æ§‹å»ºè©å½™è¡¨
        vocab_list = special_tokens + [word for word, _ in most_common]
        self.vocab = {word: idx for idx, word in enumerate(vocab_list)}
        self.idx_to_vocab = {idx: word for word, idx in self.vocab.items()}
        
    def tokenize_sentences(self):
        """å°‡å¥å­è½‰æ›ç‚ºtokenåºåˆ—"""
        tokenized = []
        for sentence in self.sentences:
            words = sentence.split()
            # æ·»åŠ BOSå’ŒEOSæ¨™è¨˜
            tokens = [self.bos_idx] + [
                self.vocab.get(word, self.unk_idx) for word in words
            ] + [self.eos_idx]
            
            # æˆªæ–·æˆ–å¡«å……åˆ°å›ºå®šé•·åº¦
            if len(tokens) > self.max_seq_len:
                tokens = tokens[:self.max_seq_len]
            else:
                tokens += [self.pad_idx] * (self.max_seq_len - len(tokens))
            
            tokenized.append(tokens)
        
        return tokenized
    
    def __len__(self):
        return len(self.tokenized_sentences)
    
    def __getitem__(self, idx):
        tokens = self.tokenized_sentences[idx]
        
        # å°æ–¼ Transformer è¨“ç·´ï¼Œæˆ‘å€‘å‰µå»ºæºåºåˆ—å’Œç›®æ¨™åºåˆ—
        # é€™è£¡æˆ‘å€‘ä½¿ç”¨ "å¾©è¿°" ä»»å‹™ï¼šæ¨¡å‹å­¸ç¿’é‡è¤‡è¼¸å…¥å¥å­
        src = torch.tensor(tokens, dtype=torch.long)
        tgt_input = torch.tensor(tokens[:-1], dtype=torch.long)  # ä¸åŒ…å«æœ€å¾Œä¸€å€‹token
        tgt_output = torch.tensor(tokens[1:], dtype=torch.long)  # ä¸åŒ…å«ç¬¬ä¸€å€‹token
        
        return src, tgt_input, tgt_output

def train_model():
    """è¨“ç·´æ¨¡å‹"""
    # è¨­å‚™è¨­ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è¨­å‚™: {device}")
    
    # è¶…åƒæ•¸
    batch_size = 16
    max_seq_len = 64  # æ¸›å°åºåˆ—é•·åº¦ä»¥åŠ å¿«è¨“ç·´
    vocab_size = 5000  # æ¸›å°è©å½™è¡¨å¤§å°
    d_model = 256     # æ¨¡å‹ç¶­åº¦
    n_heads = 8       # æ³¨æ„åŠ›é ­æ•¸
    n_layers = 4      # å±¤æ•¸ï¼ˆæ¸›å°‘ä»¥åŠ å¿«è¨“ç·´ï¼‰
    d_ff = 1024       # å‰é¥‹ç¶²çµ¡ç¶­åº¦
    dropout = 0.1
    learning_rate = 1e-4
    num_epochs = 10
    
    # å‰µå»ºæ•¸æ“šé›†
    print("å‰µå»ºæ•¸æ“šé›†...")
    dataset = TextDataset('data/input.txt', vocab_size=vocab_size, max_seq_len=max_seq_len)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    # å‰µå»ºæ¨¡å‹
    print("å‰µå»ºæ¨¡å‹...")
    model = Transformer(
        src_vocab_size=len(dataset.vocab),
        tgt_vocab_size=len(dataset.vocab),
        d_model=d_model,
        n_heads=n_heads,
        n_encoder_layers=n_layers,
        n_decoder_layers=n_layers,
        d_ff=d_ff,
        dropout=dropout,
        pad_idx=dataset.pad_idx
    ).to(device)
    
    # è¨ˆç®—åƒæ•¸æ•¸é‡
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"æ¨¡å‹åƒæ•¸æ•¸é‡: {total_params:,}")
    
    # å„ªåŒ–å™¨å’Œæå¤±å‡½æ•¸
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)
    criterion = nn.CrossEntropyLoss(ignore_index=dataset.pad_idx)
    
    # è¨“ç·´å¾ªç’°
    model.train()
    train_losses = []
    
    print("é–‹å§‹è¨“ç·´...")
    for epoch in range(num_epochs):
        epoch_losses = []
        
        pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epochs}') # å‰µå»ºé€²åº¦æ¢
        for batch_idx, (src, tgt_input, tgt_output) in enumerate(pbar):
            src = src.to(device)
            tgt_input = tgt_input.to(device)
            tgt_output = tgt_output.to(device)
            
            # å‰å‘å‚³æ’­
            optimizer.zero_grad()
            
            # ç²å–æ¨¡å‹è¼¸å‡ºï¼ˆæ³¨æ„ï¼šæ¨¡å‹è¿”å›æ¦‚ç‡åˆ†ä½ˆï¼‰
            output_probs = model(src, tgt_input)  # (batch, seq_len, vocab_size)
            
            # å°‡æ¦‚ç‡è½‰æ›å›logitsä»¥è¨ˆç®—æå¤±
            output_logits = torch.log(output_probs + 1e-8)  # æ·»åŠ å°æ•¸é¿å…log(0)
            
            # è¨ˆç®—æå¤± 
            loss = criterion(
                output_logits.reshape(-1, output_logits.size(-1)), 
                tgt_output.reshape(-1)
            )
            
            # åå‘å‚³æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª, é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            epoch_losses.append(loss.item())
            
            # æ›´æ–°é€²åº¦æ¢
            pbar.set_postfix({
                'loss': f'{loss.item():.4f}', # æå¤±
                'lr': f'{optimizer.param_groups[0]["lr"]:.2e}' # å­¸ç¿’ç‡
            })
        
        # è¨ˆç®—å¹³å‡æå¤±
        avg_loss = sum(epoch_losses) / len(epoch_losses)
        train_losses.append(avg_loss)
        
        print(f'Epoch {epoch+1}: å¹³å‡æå¤± = {avg_loss:.4f}')
        
        # æ›´æ–°å­¸ç¿’ç‡
        scheduler.step()
        
        # æ¯3å€‹epochç”Ÿæˆä¸€äº›ç¤ºä¾‹
        if (epoch + 1) % 3 == 0:
            print("\nç”Ÿæˆç¤ºä¾‹:")
            generate_samples(model, dataset, device, num_samples=2)
            print()
    
    # ä¿å­˜æ¨¡å‹
    save_path = 'transformer_model.pth'
    torch.save({
        'model_state_dict': model.state_dict(),
        'vocab': dataset.vocab,
        'idx_to_vocab': dataset.idx_to_vocab,
        'config': {
            'src_vocab_size': len(dataset.vocab),
            'tgt_vocab_size': len(dataset.vocab),
            'd_model': d_model,
            'n_heads': n_heads,
            'n_encoder_layers': n_layers,
            'n_decoder_layers': n_layers,
            'd_ff': d_ff,
            'dropout': dropout,
            'pad_idx': dataset.pad_idx
        }
    }, save_path)
    print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
    
    # ç¹ªè£½è¨“ç·´æå¤±
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses)
    plt.title('Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True)
    plt.savefig('training_loss.png')
    plt.show()
    
    return model, dataset

def generate_samples(model, dataset, device, num_samples=3):
    """ç”Ÿæˆæ–‡æœ¬ç¤ºä¾‹"""
    model.eval()
    
    with torch.no_grad():
        for i in range(num_samples):
            # éš¨æ©Ÿé¸æ“‡ä¸€å€‹è¼¸å…¥å¥å­
            idx = torch.randint(0, len(dataset), (1,))
            src, _, _ = dataset[idx.item()]
            src = src.unsqueeze(0).to(device)  # æ·»åŠ batchç¶­åº¦
            
            # ç”Ÿæˆæ–‡æœ¬
            generated = model.generate(src, max_len=32, 
                                     start_token=dataset.bos_idx, 
                                     end_token=dataset.eos_idx)
            
            # è½‰æ›ç‚ºæ–‡æœ¬
            src_text = tokens_to_text(src[0], dataset.idx_to_vocab)
            gen_text = tokens_to_text(generated[0], dataset.idx_to_vocab)
            
            print(f"ç¤ºä¾‹ {i+1}:")
            print(f"è¼¸å…¥: {src_text}")
            print(f"ç”Ÿæˆ: {gen_text}")
            print("-" * 50)
    
    model.train()

def tokens_to_text(tokens, idx_to_vocab):
    """å°‡tokenåºåˆ—è½‰æ›ç‚ºæ–‡æœ¬"""
    words = []
    for token in tokens:
        word = idx_to_vocab.get(token.item(), '<UNK>')
        if word in ['<PAD>', '<BOS>', '<EOS>']:
            if word == '<EOS>':
                break
            continue
        words.append(word)
    return ' '.join(words)

def test_model():
    """æ¸¬è©¦å·²ä¿å­˜çš„æ¨¡å‹"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # è¼‰å…¥æ¨¡å‹
    checkpoint = torch.load('transformer_model.pth', map_location=device)
    config = checkpoint['config']
    vocab = checkpoint['vocab']
    idx_to_vocab = checkpoint['idx_to_vocab']
    
    # é‡å»ºæ¨¡å‹
    model = Transformer(**config).to(device)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    print("æ¨¡å‹è¼‰å…¥æˆåŠŸï¼")
    print("ç”Ÿæˆä¸€äº›æ–‡æœ¬ç¤ºä¾‹:")
    
    # å‰µå»ºç°¡å–®çš„æ¸¬è©¦è¼¸å…¥
    test_sentences = [
        "the king is dead",
        "what is your name", 
        "i love you"
    ]
    
    model.eval()
    with torch.no_grad():
        for sentence in test_sentences:
            # ç°¡å–®çš„tokenization
            words = sentence.lower().split()
            tokens = [vocab.get('<BOS>', 2)] + [vocab.get(word, vocab.get('<UNK>', 1)) for word in words] + [vocab.get('<EOS>', 3)]
            
            # å¡«å……åˆ°åˆé©é•·åº¦
            max_len = 32
            if len(tokens) < max_len:
                tokens += [vocab.get('<PAD>', 0)] * (max_len - len(tokens))
            else:
                tokens = tokens[:max_len]
            
            src = torch.tensor(tokens).unsqueeze(0).to(device)
            
            # ç”Ÿæˆ
            generated = model.generate(src, max_len=32, 
                                     start_token=vocab.get('<BOS>', 2), 
                                     end_token=vocab.get('<EOS>', 3))
            
            # è½‰æ›ç‚ºæ–‡æœ¬
            gen_text = tokens_to_text(generated[0], idx_to_vocab)
            
            print(f"è¼¸å…¥: {sentence}")
            print(f"ç”Ÿæˆ: {gen_text}")
            print("-" * 40)

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Transformer è¨“ç·´è…³æœ¬')
    parser.add_argument('--mode', choices=['train', 'test'], default='train',
                       help='é¸æ“‡æ¨¡å¼: train (è¨“ç·´) æˆ– test (æ¸¬è©¦)')
    parser.add_argument('--push_to_hf', action='store_true',
                       help='è¨“ç·´å®Œæˆå¾Œæ¨é€åˆ° Hugging Face Hub')
    parser.add_argument('--hf_repo_name', default='shakespeare-transformer',
                       help='Hugging Face å€‰åº«åç¨±')
    parser.add_argument('--hf_username', default='ewdlop', help='Hugging Face ç”¨æˆ¶å')
    parser.add_argument('--hf_token', help='Hugging Face token (ä¸å»ºè­°åœ¨å‘½ä»¤è¡Œä¸­ä½¿ç”¨)')
    parser.add_argument('--hf_private', action='store_true',
                       help='å‰µå»ºç§æœ‰ Hugging Face å€‰åº«')
    
    args = parser.parse_args()
    
    if args.mode == 'train':
        print("é–‹å§‹è¨“ç·´ Transformer æ¨¡å‹...")
        model, dataset = train_model()
        print("è¨“ç·´å®Œæˆï¼")
        
        print("\næ¸¬è©¦è¨“ç·´å¥½çš„æ¨¡å‹:")
        generate_samples(model, dataset, torch.device('cuda' if torch.cuda.is_available() else 'cpu'), num_samples=3)
        
        # æ¨é€åˆ° Hugging Face Hub
        if args.push_to_hf:
            print("\n" + "="*50)
            print("ğŸ¤— æº–å‚™æ¨é€æ¨¡å‹åˆ° Hugging Face Hub...")
            print("="*50)
            
            try:
                from push_to_huggingface import push_to_huggingface
                
                # å®‰å…¨åœ°ç²å– token
                username = args.hf_username
                token = args.hf_token
                
                if not token:
                    print("éœ€è¦ Hugging Face token ä¾†æ¨é€æ¨¡å‹")
                    print("ğŸ’¡ æç¤º: æ‚¨å¯ä»¥åœ¨ https://huggingface.co/settings/tokens ç²å– token")
                    token = getpass.getpass("è«‹è¼¸å…¥æ‚¨çš„ Hugging Face token (è¼¸å…¥æ™‚ä¸æœƒé¡¯ç¤º): ")
                    
                    while not token.strip():
                        print("âŒ Token ä¸èƒ½ç‚ºç©ºï¼Œè«‹é‡æ–°è¼¸å…¥:")
                        token = getpass.getpass("Token: ")
                
                push_to_huggingface(
                    model_path="transformer_model.pth",
                    repo_name=args.hf_repo_name,
                    username=username,
                    token=token,
                    private=args.hf_private,
                    commit_message=f"Add {args.hf_repo_name} - Shakespeare Transformer model trained on classical text"
                )
                
                print("ğŸ‰ æ¨¡å‹å·²æˆåŠŸæ¨é€åˆ° Hugging Face Hub!")
                
            except ImportError:
                print("âŒ ç„¡æ³•å°å…¥ push_to_huggingface æ¨¡çµ„")
                print("è«‹ç¢ºä¿ push_to_huggingface.py æ–‡ä»¶å­˜åœ¨")
            except Exception as e:
                print(f"âŒ æ¨é€åˆ° Hugging Face å¤±æ•—: {str(e)}")
                print("æ‚¨å¯ä»¥ç¨å¾Œæ‰‹å‹•é‹è¡Œ:")
                print(f"python push_to_huggingface.py --repo_name {args.hf_repo_name} --interactive")
        
        elif input("\næ˜¯å¦è¦æ¨é€æ¨¡å‹åˆ° Hugging Face Hub? (y/N): ").lower().strip() == 'y':
            print("\nè¦æ¨é€åˆ° Hugging Faceï¼Œè«‹é‹è¡Œ:")
            print(f"python push_to_huggingface.py --repo_name {args.hf_repo_name} --interactive")
            print("æˆ–è€…é‡æ–°é‹è¡Œè¨“ç·´ä¸¦æ·»åŠ  --push_to_hf åƒæ•¸")
        
    elif args.mode == 'test':
        print("è¼‰å…¥ä¸¦æ¸¬è©¦æ¨¡å‹...")
        test_model() 