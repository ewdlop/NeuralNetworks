#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Mega Man LoRA å¿«é€Ÿé–‹å§‹è¨“ç·´è…³æœ¬
ä½¿ç”¨é è¨­çš„æœ€ä½³é…ç½®é€²è¡Œè¨“ç·´
"""

import subprocess
import sys
import os
from pathlib import Path
import argparse

def check_requirements():
    """æª¢æŸ¥å¿…è¦çš„ä¾è³´é …"""
    required_packages = [
        'torch', 'torchvision', 'diffusers', 'transformers', 
        'accelerate', 'peft', 'PIL', 'tqdm'
    ]
    
    missing_packages = []
    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            if package == 'PIL':
                missing_packages.append('Pillow')
            else:
                missing_packages.append(package)
    
    if missing_packages:
        print(f"âŒ ç¼ºå°‘å¿…è¦çš„ä¾è³´é …: {', '.join(missing_packages)}")
        print("è«‹é‹è¡Œ: pip install -r requirements_megaman_lora.txt")
        return False
    
    print("âœ… æ‰€æœ‰ä¾è³´é …å·²å®‰è£")
    return True

def setup_data_directory(data_dir):
    """è¨­ç½®æ•¸æ“šç›®éŒ„"""
    data_path = Path(data_dir)
    
    if not data_path.exists():
        print(f"ğŸ“ å‰µå»ºæ•¸æ“šç›®éŒ„: {data_dir}")
        data_path.mkdir(parents=True)
        
        # å‰µå»ºèªªæ˜æ–‡ä»¶
        readme_content = """
# Mega Man è¨“ç·´åœ–åƒç›®éŒ„

è«‹å°‡æ‚¨çš„ Mega Man åœ–åƒæ”¾åœ¨é€™å€‹ç›®éŒ„ä¸­ï¼š

## æ”¯æŒçš„æ ¼å¼ï¼š
- .jpg, .jpeg, .png, .bmp, .webp

## å»ºè­°ï¼š
- åœ–åƒåˆ†è¾¨ç‡ï¼š512x512 æˆ–æ›´é«˜
- åœ–åƒæ•¸é‡ï¼š20+ å¼µ (è¶Šå¤šè¶Šå¥½)
- æ¯å¼µåœ–åƒå¯ä»¥æœ‰å°æ‡‰çš„ .txt æè¿°æ–‡ä»¶

## ç¤ºä¾‹æè¿°ï¼š
- "mega man, blue robot, helmet, game character"
- "mega man X, futuristic armor, action pose"
- "robot master, colorful design, game boss"

æº–å‚™å¥½åœ–åƒå¾Œï¼Œé‡æ–°é‹è¡Œæ­¤è…³æœ¬é–‹å§‹è¨“ç·´ã€‚
"""
        with open(data_path / "README.txt", "w", encoding="utf-8") as f:
            f.write(readme_content)
        
        print(f"ğŸ“ å·²å‰µå»ºèªªæ˜æ–‡ä»¶: {data_path / 'README.txt'}")
        return False
    
    # æª¢æŸ¥æ˜¯å¦æœ‰åœ–åƒæ–‡ä»¶
    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.webp'}
    image_files = []
    for ext in image_extensions:
        image_files.extend(list(data_path.rglob(f'*{ext}')))
    
    if len(image_files) == 0:
        print(f"âŒ åœ¨ {data_dir} ä¸­æ²’æœ‰æ‰¾åˆ°åœ–åƒæ–‡ä»¶ï¼")
        print("è«‹æ·»åŠ ä¸€äº› Mega Man åœ–åƒåˆ°æ•¸æ“šç›®éŒ„ä¸­")
        return False
    
    print(f"âœ… æ‰¾åˆ° {len(image_files)} å¼µåœ–åƒï¼Œå¯ä»¥é–‹å§‹è¨“ç·´")
    return True

def run_training(args):
    """é‹è¡Œè¨“ç·´"""
    print("ğŸš€ é–‹å§‹ Mega Man LoRA è¨“ç·´...")
    
    # æ§‹å»ºè¨“ç·´å‘½ä»¤
    cmd = [
        sys.executable, "megaman_lora.py",
        "--data_dir", args.data_dir,
        "--output_dir", args.output_dir,
        "--resolution", str(args.resolution),
        "--train_batch_size", str(args.batch_size),
        "--gradient_accumulation_steps", str(args.grad_accum),
        "--learning_rate", str(args.learning_rate),
        "--lora_rank", str(args.lora_rank),
        "--lora_alpha", str(args.lora_alpha),
        "--max_train_steps", str(args.max_steps),
        "--save_steps", str(args.save_steps),
        "--validation_steps", str(args.validation_steps),
        "--mixed_precision", args.mixed_precision,
    ]
    
    if args.seed:
        cmd.extend(["--seed", str(args.seed)])
    
    print(f"ğŸ“ è¨“ç·´å‘½ä»¤: {' '.join(cmd)}")
    print("=" * 60)
    
    # é‹è¡Œè¨“ç·´
    try:
        subprocess.run(cmd, check=True)
        print("=" * 60)
        print("âœ… è¨“ç·´å®Œæˆï¼")
        print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {args.output_dir}")
        
        # æç¤ºä¸‹ä¸€æ­¥
        print("\nğŸ¨ ç¾åœ¨æ‚¨å¯ä»¥ç”Ÿæˆåœ–åƒ:")
        print(f"python generate_megaman.py --lora_path {args.output_dir}/lora_weights")
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ è¨“ç·´å¤±æ•—: {e}")
        return False
    except KeyboardInterrupt:
        print("\nâ¹ï¸ è¨“ç·´è¢«ç”¨æˆ¶ä¸­æ–·")
        return False
    
    return True

def main():
    parser = argparse.ArgumentParser(description="Mega Man LoRA å¿«é€Ÿè¨“ç·´")
    
    # åŸºæœ¬è¨­ç½®
    parser.add_argument("--data_dir", type=str, default="./megaman_images",
                       help="è¨“ç·´æ•¸æ“šç›®éŒ„ (é è¨­: ./megaman_images)")
    parser.add_argument("--output_dir", type=str, default="./megaman_lora_output",
                       help="è¼¸å‡ºç›®éŒ„ (é è¨­: ./megaman_lora_output)")
    
    # è¨“ç·´é…ç½®é è¨­ (åˆå­¸è€…å‹å¥½)
    parser.add_argument("--config", type=str, default="balanced",
                       choices=["fast", "balanced", "quality"],
                       help="è¨“ç·´é…ç½®é è¨­")
    
    # å¯é¸çš„è‡ªå®šç¾©åƒæ•¸
    parser.add_argument("--resolution", type=int, default=None,
                       help="åœ–åƒåˆ†è¾¨ç‡ (é è¨­æœƒæ ¹æ“šé…ç½®é¸æ“‡)")
    parser.add_argument("--batch_size", type=int, default=None,
                       help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--max_steps", type=int, default=None,
                       help="æœ€å¤§è¨“ç·´æ­¥æ•¸")
    parser.add_argument("--learning_rate", type=float, default=None,
                       help="å­¸ç¿’ç‡")
    parser.add_argument("--seed", type=int, default=42,
                       help="éš¨æ©Ÿç¨®å­ (é è¨­: 42)")
    
    args = parser.parse_args()
    
    # æ ¹æ“šé…ç½®è¨­ç½®é è¨­å€¼
    configs = {
        "fast": {
            "resolution": 512,
            "batch_size": 1,
            "grad_accum": 2,
            "learning_rate": 1e-4,
            "lora_rank": 4,
            "lora_alpha": 16,
            "max_steps": 800,
            "save_steps": 200,
            "validation_steps": 100,
            "mixed_precision": "fp16",
        },
        "balanced": {
            "resolution": 512,
            "batch_size": 1,
            "grad_accum": 4,
            "learning_rate": 8e-5,
            "lora_rank": 8,
            "lora_alpha": 32,
            "max_steps": 1500,
            "save_steps": 250,
            "validation_steps": 100,
            "mixed_precision": "fp16",
        },
        "quality": {
            "resolution": 768,
            "batch_size": 1,
            "grad_accum": 8,
            "learning_rate": 5e-5,
            "lora_rank": 16,
            "lora_alpha": 64,
            "max_steps": 2500,
            "save_steps": 300,
            "validation_steps": 150,
            "mixed_precision": "fp16",
        }
    }
    
    config = configs[args.config]
    
    # æ‡‰ç”¨é è¨­å€¼ (å¦‚æœæ²’æœ‰æ‰‹å‹•æŒ‡å®š)
    for key, value in config.items():
        if not hasattr(args, key) or getattr(args, key) is None:
            setattr(args, key, value)
    
    print("ğŸ¤– Mega Man LoRA å¿«é€Ÿè¨“ç·´åŠ©æ‰‹ ğŸ®")
    print("=" * 50)
    print(f"ğŸ“Š é…ç½®: {args.config}")
    print(f"ğŸ“ æ•¸æ“šç›®éŒ„: {args.data_dir}")
    print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {args.output_dir}")
    print(f"ğŸ–¼ï¸  åˆ†è¾¨ç‡: {args.resolution}")
    print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"ğŸ”„ æ¢¯åº¦ç´¯ç©: {args.grad_accum}")
    print(f"ğŸ“ˆ å­¸ç¿’ç‡: {args.learning_rate}")
    print(f"ğŸ”¢ LoRA Rank: {args.lora_rank}")
    print(f"ğŸ”¢ LoRA Alpha: {args.lora_alpha}")
    print(f"ğŸ¯ æœ€å¤§æ­¥æ•¸: {args.max_steps}")
    print("=" * 50)
    
    # æª¢æŸ¥ä¾è³´é …
    if not check_requirements():
        return
    
    # è¨­ç½®æ•¸æ“šç›®éŒ„
    if not setup_data_directory(args.data_dir):
        return
    
    # é–‹å§‹è¨“ç·´
    run_training(args)

if __name__ == "__main__":
    main()

"""
ä½¿ç”¨ç¤ºä¾‹:

1. å¿«é€Ÿè¨“ç·´ (é©åˆæ¸¬è©¦):
python train_quick_start.py --config fast

2. å¹³è¡¡è¨“ç·´ (æ¨è–¦):
python train_quick_start.py --config balanced

3. é«˜è³ªé‡è¨“ç·´ (éœ€è¦æ›´å¤šæ™‚é–“å’Œè³‡æº):
python train_quick_start.py --config quality

4. è‡ªå®šç¾©é…ç½®:
python train_quick_start.py --config balanced --max_steps 2000 --resolution 768

5. æŒ‡å®šæ•¸æ“šç›®éŒ„:
python train_quick_start.py --data_dir ./my_megaman_images --output_dir ./my_lora_output
""" 