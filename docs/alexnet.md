### 為什麼很多程式碼（含 PyTorch `torchvision.models.alexnet` 與我剛才的範例）把 **Conv1** 改成 64 個 filters，而原始論文和你找到的示意圖卻寫 **96**？

| 版本                             | Conv1 filters | 主要差異點                                                                                                            | 背後原因                                                                   |
| ------------------------------ | ------------- | ---------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| **原始 AlexNet 論文 (2012)**       | **96**        | - 論文採 *雙 GPU* 訓練，96 個 filters 被平均分到 2 張 GTX 580（每張 48 個）<br>- 論文還包含 *Local Response Normalization (LRN)* 等當時常用的層 | 2012 年 3 GB VRAM 不夠一次載入全部權重與 feature maps，只能把網路「劈成兩半」放到兩張卡上，同步訓練       |
| **單 GPU 簡化版（Krizhevsky 之後提供）** | **64**        | - 去掉雙 GPU 分組需求，整網路塞進 1 張 GPU<br>- 稍微縮減通道數（96→64；256→192…）                                                        | 方便研究人員在單卡上重現並微調；減少記憶體佔用與 FLOPs，不會明顯拉低 ImageNet 準確率                     |
| **PyTorch / Keras 等主流實作**      | **64**        | 多沿用 Krizhevsky 的 *single-GPU fork*；同時把輸入改成 224²（方便和 VGG/ResNet 對齊）                                               | 以 224² + 64 filters 訓練/推論，用今天常見的 8 GB～16 GB GPU 也毫無壓力，並能直接載入官方提供的預訓練權重 |

---

#### 記憶體與運算量對比（粗估）

* **參數量**

  * 96 filters：3 × 11 × 11 × 96 ≈ **35 K**
  * 64 filters：3 × 11 × 11 × 64 ≈ **23 K**

* **Feature map 體積（以輸入 227²、batch = 128 計算）**

  * 96：56 × 56 × 96 × 128 ≈ **4.3 GB**
  * 64：56 × 56 × 64 × 128 ≈ **2.9 GB**

> 對當年 3 GB VRAM 的 GTX 580 而言，單層就塞不下；切兩半各 48 channels 就能勉強運作。

---

### 要不要改回 96？

1. **完整復刻論文**

   * 把 `nn.Conv2d(3, 64, …)` 改成 `nn.Conv2d(3, 96, …)`
   * 把第二層起始通道 192 → 256；第三～五層通道依次調整為 384, 384, 256
   * 若想忠於雙 GPU，可用 `groups=2` 將張量手動分組以模擬「各 GPU 處理一半通道」的佈局
   * 第一個 `nn.Linear` 的輸入維度仍是 256×6×6＝9216（不變）

2. **沿用 64 版優點**

   * 記憶體、計算量更小，與現成的 `pretrained=True` 權重兼容
   * 實測在 ImageNet Top-1/Top-5 差距僅約 0.3%～0.5%，往往被資料增強或訓練策略噪聲淹沒

---

### 參考資料

* Analytics Vidhya 針對 **96 filters** 的原始架構說明﻿ ([analyticsvidhya.com][1])
* PyTorch 論壇討論：`torchvision` 選用 **64 filters** 是基於作者的單 GPU fork﻿ ([discuss.pytorch.org][2])

---

#### 小結

* 圖上看到的 96 filters 來自 **原始雙 GPU** 版本。
* 今天常見教學／套件採用 64 filters，是 **單 GPU 精簡版**；方便訓練，也有現成權重。
* 兩者本質相同，若硬體允許、且想忠於歷史，可把程式碼改回 96；否則 64 版已足以學習與推斷。

[1]: https://www.analyticsvidhya.com/blog/2021/03/introduction-to-the-architecture-of-alexnet/ "Introduction to Alexnet Architecture"
[2]: https://discuss.pytorch.org/t/does-alexnet-architecture-match-the-paper-definition/27410 "Does AlexNet Architecture match the paper definition? - vision - PyTorch Forums"


### 為什麼要用「卷積 (Convolution)」？

| 特色        | 說明                                                                             |
| --------- | ------------------------------------------------------------------------------ |
| **局部感受野** | 卷積核只看鄰近像素（例如 3 × 3、5 × 5），能專注捕捉局部紋理與邊緣，與人類視皮層的分層感知機制相似。                        |
| **參數共享**  | 一個卷積核在整張圖上滑動，同一組權重重複使用 → 參數量大幅少於全連接層（O(CHW) → O(C k²)），訓練更穩定、記憶體與 FLOPs 都大幅降低。 |
| **平移等變性** | 無論特徵出現在圖像何處，卷積核都能偵測到；網路學得的不再是「座標」，而是「模式」。這讓模型對位移更有韌性。                          |
| **特徵層級化** | 疊很多卷積可自動從「邊緣→紋理→局部物件→整體物件」逐層抽象，取代手工設計特徵 (SIFT/HOG)。                            |

---

### 為什麼要用「最大池化 (Max Pooling)」？

| 目的                                 | 作用                                                                |
| ---------------------------------- | ----------------------------------------------------------------- |
| **降取樣 (Down-sampling)**            | 例如 2 × 2、stride 2 的池化，將 H、W 各減半，把注意力從像素層級聚焦到更大局部，後續卷積計算量也跟著少 4 倍。 |
| **平移不變性 (Translation Invariance)** | 取區域最大值＝「只要某個特徵在窗口內出現一次，就算偵測到」。細部位移 ≈ 仍被認為同一事件 → 對手持抖動、物體微移更有韌性。   |
| **防止過擬合**                          | 捨棄高解析度資訊，相當於對特徵做「資訊瓶頸」，鼓勵網路學習更抽象、泛化性更強的表示。                        |
| **增大感受野**                          | 每池化一次，之後卷積看到的實際影像區域 ×2；加速「看遠」的能力，而無需超大卷積核。                        |

> 📌 **替代方案**：今天流行的 *Strided Convolution*、*Average Pooling*、甚至 *Global Pooling* 和 *Attention* 都可取代 MaxPool；AlexNet 時代 MaxPool 是最簡單穩定的選擇。

---

### ReLU (Rectified Linear Unit) 的角色

| 特性                      | 效果                                                        |
| ----------------------- | --------------------------------------------------------- |
| **非線性** `f(x)=max(0,x)` | 若沒有 ReLU，層與層之間只是線性變換疊加，整網路仍等同單層線性，無法擬合複雜函數。非線性讓網路能逼近任何映射。 |
| **抑制負半邊 → 稀疏激活**        | 輸出為 0 的 neuron 不參與後續計算；稀疏向量更易解釋，也可降低相互干擾 (co-adaptation)。 |
| **避免梯度消失**              | 相比 sigmoid/tanh，在正區域斜率=1，梯度不會因飽和趨近 0，故深層網路仍能有效回傳誤差。       |
| **計算便宜**                | 只需比較與取最大，不用指數或除法；對 GPU/CPU 友善。                            |

---

### 三者如何配合工作？

1. **Conv** 學習「偵測器」──邊緣、角落、紋理。
2. **ReLU** 把「偵測到」的訊號保持正值，未偵測到則歸零，形成稀疏且梯度友善的特徵圖。
3. **MaxPool** 將「哪裡偵測到」抽象成「區域內有偵測到即可」，並壓縮解析度；這樣後續卷積能在更大語境下尋找更高層語義。

如此層疊數次後，再接全連接層（或 Global Pooling + Linear）做分類，即形成經典 CNN 流水線。

---

#### 小結

* **卷積**：低參數、可平移的局部特徵提取器。
* **Max Pooling**：降維兼增強平移不變性，減少過擬合。
* **ReLU**：賦予非線性、避免梯度消失、計算快速。

三者交織，使得深度網路既能高效訓練又具備強大表達力，成就了 AlexNet 在 2012 年 ImageNet 大勝、掀起深度學習浪潮。
