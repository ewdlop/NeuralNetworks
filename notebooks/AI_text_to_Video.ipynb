{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDZ1cweXj8s3"
      },
      "source": [
        "Sora from OpenAI, Stable Video Diffusion from Stability AI, and many other text-to-video models that have come out or will appear in the future are among the most popular AI trends in 2024, following large language models (LLMs). In this blog, we will build a **small scale text-to-video model from scratch**. We will input a text prompt, and our trained model will generate a video based on that prompt. This blog will cover everything from understanding the theoretical concepts to coding the entire architecture and generating the final result.\n",
        "\n",
        "Since I don’t have a fancy GPU, I’ve coded the small-scale architecture. Here’s a comparison of the time required to train the model on different processors:\n",
        "\n",
        "| Training Videos | Epochs | CPU      | GPU A10 | GPU T4    |\n",
        "|---------------|--------|----------|---------|-----------|\n",
        "| 10K           | 30     | more than 3 hr    | 1 hr    | 1 hr 42m  |\n",
        "| 30K           | 30     | more than 6 hr    | 1 hr 30 | 2 hr 30   |\n",
        "| 100K          | 30     | -        | 3-4 hr  | 5-6 hr    |\n",
        "\n",
        "Running on a CPU will obviously take much longer to train the model. If you need to quickly test changes in the code and see results, CPU is not the best choice. I recommend using a T4 GPU from [Colab](https://colab.research.google.com/) or [Kaggle](https://kaggle.com/) for more efficient and faster training.\n",
        "\n",
        "Here is the blog link which guides you on how to create Stable Diffusion from scratch:\n",
        "[Coding Stable Diffusion from Scratch](https://levelup.gitconnected.com/building-stable-diffusion-from-scratch-using-python-f3ebc8c42da3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BnTekhJj8s6"
      },
      "source": [
        "## What We’re Building\n",
        "\n",
        "We will follow a similar approach to traditional machine learning or deep learning models that train on a dataset and are then tested on unseen data. In the context of text-to-video, let’s say we have a training dataset of 100K videos of dogs fetching balls and cats chasing mice. We will train our model to generate videos of a cat fetching a ball or a dog chasing a mouse.\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/3840/1*6h3oJzGEH0xrER2Tv8M7KQ.gif\" width=\"900\">\n",
        "\n",
        "Although such training datasets are easily available on the internet, the required computational power is extremely high. Therefore, we will work with a video dataset of moving objects generated from Python code.\n",
        "\n",
        "We will use the GAN (Generative Adversarial Networks) architecture to create our model instead of the diffusion model that OpenAI Sora uses. I attempted to use the diffusion model, but it crashed due to memory requirements, which is beyond my capacity. GANs, on the other hand, are easier and quicker to train and test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zSgLUkmj8s6"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "We will be using OOP (Object-Oriented Programming), so you must have a basic understanding of it along with neural networks. Knowledge of GANs (Generative Adversarial Networks) is not mandatory, as we will be covering their architecture here.\n",
        "\n",
        "| Topic | Link |\n",
        "| ---- | ---- |\n",
        "| OOP | [Video Link](https://www.youtube.com/watch?v=q2SGW2VgwAM) |\n",
        "| Neural Networks Theory |  [Video Link](https://www.youtube.com/watch?v=Jy4wM2X21u0) |\n",
        "| GAN Architecture |  [Video Link](https://www.youtube.com/watch?v=TpMIssRdhco) |\n",
        "| Python basics |  [Video Link](https://www.youtube.com/watch?v=eWRfhZUzrAc) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNm7BMx4j8s6"
      },
      "source": [
        "# Understanding the GAN Architecture\n",
        "\n",
        "Understanding GAN architecture is important because much of our architecture depends on it. Let’s explore what it is, its components, and more.\n",
        "\n",
        "### what is **GAN**?\n",
        "\n",
        "Generative Adversarial Network (GAN) is a deep learning model where two neural networks compete: one creates new data (like images or music) from a given dataset, and the other tries to tell if the data is real or fake. This process continues until the generated data is indistinguishable from the original.\n",
        "\n",
        "### Real-World Application\n",
        "\n",
        " 1. **Generate Images**: GANs create realistic images from text prompts or modify existing images, such as enhancing resolution or adding color to black-and-white photos.\n",
        "\n",
        " 2. **Data Augmentation**: They generate synthetic data to train other machine learning models, such as creating fraudulent transaction data for fraud detection systems.\n",
        "\n",
        " 3. **Complete Missing Information**: GANs can fill in missing data, like generating sub-surface images from terrain maps for energy applications.\n",
        "\n",
        " 4. **Generate 3D Models**: They convert 2D images into 3D models, useful in fields like healthcare for creating realistic organ images for surgical planning.\n",
        "\n",
        "### How does a GAN work?\n",
        "\n",
        "It consists of two deep neural networks: the **generator** and the **discriminator**. These networks train together in an adversarial setup, where one generates new data and the other evaluates if the data is real or fake.\n",
        "\n",
        "Here’s a simplified overview of how GAN works:\n",
        "\n",
        " 1. **Training Set Analysis**: The generator analyzes the training set to identify data attributes, while the discriminator independently analyzes the same data to learn its attributes.\n",
        "\n",
        " 2. **Data Modification**: The generator adds noise (random changes) to some attributes of the data.\n",
        "\n",
        " 3. **Data Passing**: The modified data is then passed to the discriminator.\n",
        "\n",
        " 4. **Probability Calculation**: The discriminator calculates the probability that the generated data is from the original dataset.\n",
        "\n",
        " 5. **Feedback Loop**: The discriminator provides feedback to the generator, guiding it to reduce random noise in the next cycle.\n",
        "\n",
        " 6. **Adversarial Training**: The generator tries to maximize the discriminator’s mistakes, while the discriminator tries to minimize its own errors. Through many training iterations, both networks improve and evolve.\n",
        "\n",
        " 7. **Equilibrium State**: Training continues until the discriminator can no longer distinguish between real and synthesized data, indicating that the generator has successfully learned to produce realistic data. At this point, the training process is complete.\n",
        "\n",
        "![From [AWS Guide](https://aws.amazon.com/what-is/gan/)](https://cdn-images-1.medium.com/max/2796/1*2HsK-UFPRvCdAmQyS3Ol1Q.jpeg)\n",
        "\n",
        "### GAN training example\n",
        "\n",
        "Let’s explain the GAN model with an example of image-to-image translation, focusing on modifying a human face.\n",
        "\n",
        " 1. **Input Image**: The input is a real image of a human face.\n",
        "\n",
        " 2. **Attribute Modification**: The generator modifies attributes of the face, like adding sunglasses to the eyes.\n",
        "\n",
        " 3. **Generated Images**: The generator creates a set of images with sunglasses added.\n",
        "\n",
        " 4. **Discriminator’s Task**: The discriminator receives a mix of real images (people with sunglasses) and generated images (faces where sunglasses were added).\n",
        "\n",
        " 5. **Evaluation**: The discriminator tries to differentiate between real and generated images.\n",
        "\n",
        " 6. **Feedback Loop**: If the discriminator correctly identifies fake images, the generator adjusts its parameters to produce more convincing images. If the generator successfully fools the discriminator, the discriminator updates its parameters to improve its detection.\n",
        "\n",
        "Through this adversarial process, both networks continuously improve. The generator gets better at creating realistic images, and the discriminator gets better at identifying fakes until equilibrium is reached, where the discriminator can no longer tell the difference between real and generated images. At this point, the GAN has successfully learned to produce realistic modifications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFFrVzR9j8s7"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_vABkDvGj8s7"
      },
      "outputs": [],
      "source": [
        "# Operating System module for interacting with the operating system\n",
        "# 作業系統模組，用於與作業系統互動\n",
        "import os\n",
        "\n",
        "# Module for generating random numbers\n",
        "# 隨機數模組，用於生成隨機數\n",
        "import random\n",
        "\n",
        "# Module for numerical operations\n",
        "# 數值運算模組，用於進行數值運算\n",
        "import numpy as np\n",
        "\n",
        "# OpenCV library for image processing\n",
        "# OpenCV 庫，用於圖像處理\n",
        "import cv2\n",
        "\n",
        "# Python Imaging Library for image processing\n",
        "# Python 影像處理庫，用於圖像處理\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "# PyTorch library for deep learning\n",
        "# PyTorch 庫，用於深度學習\n",
        "import torch\n",
        "\n",
        "# Dataset class for creating custom datasets in PyTorch\n",
        "# PyTorch 資料集類，用於創建自定義資料集\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Module for image transformations\n",
        "# 圖像轉換模組，用於圖像轉換\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Neural network module in PyTorch\n",
        "# PyTorch 神經網絡模組，用於創建神經網絡\n",
        "import torch.nn as nn\n",
        "\n",
        "# Optimization algorithms in PyTorch\n",
        "# PyTorch 優化算法模組，用於優化模型\n",
        "import torch.optim as optim\n",
        "\n",
        "# Function for padding sequences in PyTorch\n",
        "# PyTorch 序列填充函數，用於填充序列\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Function for saving images in PyTorch\n",
        "# PyTorch 圖像保存函數，用於保存圖像\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# Module for plotting graphs and images\n",
        "# 圖表和圖像繪製模組，用於繪製圖表和圖像\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Module for displaying rich content in IPython environments\n",
        "# IPython 環境中的豐富內容顯示模組，用於顯示豐富內容\n",
        "from IPython.display import clear_output, display, HTML\n",
        "\n",
        "# Module for encoding and decoding binary data to text\n",
        "# 二進制數據編碼和解碼模組，用於編碼和解碼二進制數據\n",
        "import base64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mg5sx3Qnj8s8"
      },
      "source": [
        "## Coding the Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Lh9GLYXj8s8",
        "outputId": "0285032c-b1cd-4ab7-ba35-311484b27b52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset generation complete.\n"
          ]
        }
      ],
      "source": [
        "# Create the directory for the training dataset, 創建訓練資料集的目錄\n",
        "os.makedirs('training_dataset', exist_ok=True)\n",
        "\n",
        "# Define the number of videos to generate for the dataset, 定義要生成的視頻數量 \n",
        "num_videos = 30000\n",
        "\n",
        "# Define the number of frames per video (1 Second Video), 定義每個視頻的幀數 (1 秒視頻)\n",
        "frames_per_video = 10\n",
        "\n",
        "# Define the size of each image in the dataset, 定義資料集中的每個圖像的大小\n",
        "img_size = (64, 64)\n",
        "\n",
        "# Define the size of the shapes (Circle), 定義圖形的大小 (圓形)\n",
        "shape_size = 10\n",
        "\n",
        "# Define text prompts and corresponding movements for circles\n",
        "prompts_and_movements = [\n",
        "    (\"circle moving down\", \"circle\", \"down\"),  # Move circle downward, 圓形向下移動\n",
        "    (\"circle moving left\", \"circle\", \"left\"),  # Move circle leftward, 圓形向左移動 \n",
        "    (\"circle moving right\", \"circle\", \"right\"),  # Move circle rightward, 圓形向右移動  \n",
        "    (\"circle moving diagonally up-right\", \"circle\", \"diagonal_up_right\"),  # Move circle diagonally up-right, 圓形對角線向上右移動\n",
        "    (\"circle moving diagonally down-left\", \"circle\", \"diagonal_down_left\"),  # Move circle diagonally down-left, 圓形對角線向下左移動\n",
        "    (\"circle moving diagonally up-left\", \"circle\", \"diagonal_up_left\"),  # Move circle diagonally up-left, 圓形對角線向上左移動\n",
        "    (\"circle moving diagonally down-right\", \"circle\", \"diagonal_down_right\"),  # Move circle diagonally down-right, 圓形對角線向下右移動\n",
        "    (\"circle rotating clockwise\", \"circle\", \"rotate_clockwise\"),  # Rotate circle clockwise, 圓形順時針旋轉\n",
        "    (\"circle rotating counter-clockwise\", \"circle\", \"rotate_counter_clockwise\"),  # Rotate circle counter-clockwise, 圓形逆時針旋轉\n",
        "    (\"circle bouncing vertically\", \"circle\", \"bounce_vertical\"),  # Bounce circle vertically, 圓形垂直彈跳\n",
        "    (\"circle bouncing horizontally\", \"circle\", \"bounce_horizontal\"),  # Bounce circle horizontally, 圓形水平彈跳\n",
        "    (\"circle zigzagging vertically\", \"circle\", \"zigzag_vertical\"),  # Zigzag circle vertically, 圓形垂直之字形\n",
        "    (\"circle zigzagging horizontally\", \"circle\", \"zigzag_horizontal\"),  # Zigzag circle horizontally, 圓形水平之字形\n",
        "    (\"circle moving up-left\", \"circle\", \"up_left\"),  # Move circle up-left, 圓形向上左移動\n",
        "    (\"circle moving down-right\", \"circle\", \"down_right\"),  # Move circle down-right, 圓形向下右移動\n",
        "    (\"circle moving down-left\", \"circle\", \"down_left\")  # Move circle down-left, 圓形向下左移動\n",
        "]\n",
        "\n",
        "# Define a function to create an image with a moving shape, 定義一個函數來創建一個帶有移動圖形的圖像\n",
        "def create_image_with_moving_shape(size, frame_num, shape, direction):\n",
        "    # Create a new RGB image with the specified size and white background, 創建一個新的 RGB 圖像，大小為指定的大小，背景為白色\n",
        "    img = Image.new('RGB', size, color=(255, 255, 255)) \n",
        "    draw = ImageDraw.Draw(img) # 創建一個繪圖對象\n",
        "\n",
        "    # Calculate the initial position of the shape (center of the image), 計算圖形的初始位置 (圖像的中心)\n",
        "    center_x, center_y = size[0] // 2, size[1] // 2\n",
        "\n",
        "    # Determine the shape position based on the movement direction, 根據移動方向確定圖形的位置\n",
        "    if direction == \"down\":\n",
        "        position = (center_x, (center_y + frame_num * 5) % size[1])\n",
        "    elif direction == \"left\":\n",
        "        position = ((center_x - frame_num * 5) % size[0], center_y)\n",
        "    elif direction == \"right\":\n",
        "        position = ((center_x + frame_num * 5) % size[0], center_y)\n",
        "    elif direction == \"diagonal_up_right\":\n",
        "        position = ((center_x + frame_num * 5) % size[0], (center_y - frame_num * 5) % size[1])\n",
        "    elif direction == \"diagonal_down_left\":\n",
        "        position = ((center_x - frame_num * 5) % size[0], (center_y + frame_num * 5) % size[1])\n",
        "    elif direction == \"diagonal_up_left\":\n",
        "        position = ((center_x - frame_num * 5) % size[0], (center_y - frame_num * 5) % size[1])\n",
        "    elif direction == \"diagonal_down_right\":\n",
        "        position = ((center_x + frame_num * 5) % size[0], (center_y + frame_num * 5) % size[1])\n",
        "    elif direction == \"rotate_clockwise\":\n",
        "        img = img.rotate(frame_num * 10, center=(center_x, center_y), fillcolor=(255, 255, 255))\n",
        "        position = (center_x, center_y)\n",
        "    elif direction == \"rotate_counter_clockwise\":\n",
        "        img = img.rotate(-frame_num * 10, center=(center_x, center_y), fillcolor=(255, 255, 255))\n",
        "        position = (center_x, center_y)\n",
        "    elif direction == \"bounce_vertical\":\n",
        "        position = (center_x, center_y - abs(frame_num * 5 % size[1] - center_y))\n",
        "    elif direction == \"bounce_horizontal\":\n",
        "        position = (center_x - abs(frame_num * 5 % size[0] - center_x), center_y)\n",
        "    elif direction == \"zigzag_vertical\":\n",
        "        position = (center_x, center_y - frame_num * 5 % size[1] if frame_num % 2 == 0 else center_y + frame_num * 5 % size[1])\n",
        "    elif direction == \"zigzag_horizontal\":\n",
        "        position = (center_x - frame_num * 5 % size[0] if frame_num % 2 == 0 else center_x + frame_num * 5 % size[0], center_y)\n",
        "    elif direction == \"up_left\":\n",
        "        position = ((center_x - frame_num * 5) % size[0], (center_y - frame_num * 5) % size[1])\n",
        "    elif direction == \"down_right\":\n",
        "        position = ((center_x + frame_num * 5) % size[0], (center_y + frame_num * 5) % size[1])\n",
        "    elif direction == \"down_left\":\n",
        "        position = ((center_x - frame_num * 5) % size[0], (center_y + frame_num * 5) % size[1])\n",
        "    else:\n",
        "        position = (center_x, center_y)\n",
        "\n",
        "    # Draw the shape (circle) at the calculated position, 在計算出的位置繪製圖形 (圓形)\n",
        "    if shape == \"circle\":\n",
        "        draw.ellipse([position[0] - shape_size // 2, position[1] - shape_size // 2, position[0] + shape_size // 2, position[1] + shape_size // 2], fill=(0, 0, 255))\n",
        "\n",
        "    # Return the image as a numpy array\n",
        "    return np.array(img)\n",
        "\n",
        "# Generate the dataset, 生成資料集\n",
        "for video_num in range(num_videos): # 遍歷每個視頻\n",
        "    prompt, shape, direction = random.choice(prompts_and_movements) # 隨機選擇一個提示、圖形和方向\n",
        "    video_frames = [] # 初始化視頻幀列表\n",
        "    for frame_num in range(frames_per_video): # 遍歷每個幀\n",
        "        img_array = create_image_with_moving_shape(img_size, frame_num, shape, direction) # 創建一個帶有移動圖形的圖像\n",
        "        video_frames.append(img_array)\n",
        "\n",
        "    # Save the frames as images in the training dataset directory, 將幀保存為圖像，存儲在訓練資料集目錄中\n",
        "    video_dir = os.path.join('training_dataset', f'video_{video_num}') # 創建視頻目錄\n",
        "    os.makedirs(video_dir, exist_ok=True) # 創建視頻目錄\n",
        "    for frame_num, frame in enumerate(video_frames): # 遍歷每個幀\n",
        "        frame_image = Image.fromarray(frame) # 將幀轉換為圖像\n",
        "        frame_image.save(os.path.join(video_dir, f'frame_{frame_num}.png')) # 保存圖像\n",
        "\n",
        "print(\"Dataset generation complete.\") # 資料集生成完成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IF10vxGej8s8"
      },
      "outputs": [],
      "source": [
        "# Iterate over the number of videos to generate, 遍歷要生成的視頻數量\n",
        "for i in range(num_videos):\n",
        "    # Randomly choose a prompt and movement from the predefined list, 隨機選擇一個提示和移動\n",
        "    prompt, shape, direction = random.choice(prompts_and_movements)\n",
        "\n",
        "    # Create a directory for the current video, 創建當前視頻的目錄\n",
        "    video_dir = f'training_dataset/video_{i}' # 創建視頻目錄\n",
        "    os.makedirs(video_dir, exist_ok=True) # 創建視頻目錄\n",
        "\n",
        "    # Write the chosen prompt to a text file in the video directory\n",
        "    with open(f'{video_dir}/prompt.txt', 'w') as f: # 將選擇的提示寫入視頻目錄中的文本文件\n",
        "        f.write(prompt) # 寫入提示\n",
        "\n",
        "    # Generate frames for the current video\n",
        "    for frame_num in range(frames_per_video): # 遍歷每個幀\n",
        "        # Create an image with a moving shape based on the current frame number, shape, and direction, 基於當前幀數、圖形和方向創建一個帶有移動圖形的圖像\n",
        "        img = create_image_with_moving_shape(img_size, frame_num, shape, direction) # 創建一個帶有移動圖形的圖像\n",
        "\n",
        "        # Save the generated image as a PNG file in the video directory\n",
        "        cv2.imwrite(f'{video_dir}/frame_{frame_num}.png', img) # 將生成的圖像保存為 PNG 文件，存儲在視頻目錄中"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXLQf-Wkj8s9"
      },
      "source": [
        "## Pre-Processing Our Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlSlYNKpj8s9"
      },
      "outputs": [],
      "source": [
        "# Define a dataset class inheriting from torch.utils.data.Dataset\n",
        "class TextToVideoDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        # Initialize the dataset with root directory and optional transform\n",
        "        self.root_dir = root_dir # 初始化資料集的根目錄\n",
        "        self.transform = transform # 初始化轉換\n",
        "        # List all subdirectories in the root directory, 列出根目錄下的所有子目錄\n",
        "        self.video_dirs = [os.path.join(root_dir, d) for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))] # 列出根目錄下的所有子目錄\n",
        "        # Initialize lists to store frame paths and corresponding prompts, 初始化列表來存儲幀路徑和相應的提示\n",
        "        self.frame_paths = [] # 初始化幀路徑列表\n",
        "        self.prompts = [] # 初始化提示列表  \n",
        "\n",
        "        # Loop through each video directory, 遍歷每個視頻目錄\n",
        "        for video_dir in self.video_dirs: # 遍歷每個視頻目錄\n",
        "            # List all PNG files in the video directory and store their paths, 列出視頻目錄下的所有 PNG 文件，並存儲其路徑\n",
        "            frames = [os.path.join(video_dir, f) for f in os.listdir(video_dir) if f.endswith('.png')] # 列出視頻目錄下的所有 PNG 文件，並存儲其路徑\n",
        "            self.frame_paths.extend(frames) # 將幀路徑列表擴展為視頻目錄下的所有 PNG 文件的路徑\n",
        "            # Read the prompt text file in the video directory and store its content, 讀取視頻目錄下的提示文本文件，並存儲其內容\n",
        "            with open(os.path.join(video_dir, 'prompt.txt'), 'r') as f: # 讀取視頻目錄下的提示文本文件\n",
        "                prompt = f.read().strip() # 讀取視頻目錄下的提示文本文件的內容\n",
        "            # Repeat the prompt for each frame in the video and store in prompts list, 重複提示，將其存儲在提示列表中\n",
        "            self.prompts.extend([prompt] * len(frames)) # 將提示列表擴展為視頻目錄下的所有 PNG 文件的提示\n",
        "\n",
        "    # Return the total number of samples in the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.frame_paths) # 返回資料集中的幀路徑數量\n",
        "\n",
        "    # Retrieve a sample from the dataset given an index\n",
        "    def __getitem__(self, idx): # 獲取資料集中的樣本，給定索引\n",
        "        # Get the path of the frame corresponding to the given index, 獲取與給定索引對應的幀路徑\n",
        "        frame_path = self.frame_paths[idx] # 獲取與給定索引對應的幀路徑\n",
        "        # Open the image using PIL (Python Imaging Library), 使用 PIL (Python Imaging Library) 打開圖像\n",
        "        image = Image.open(frame_path) # 使用 PIL (Python Imaging Library) 打開圖像\n",
        "        # Get the prompt corresponding to the given index, 獲取與給定索引對應的提示\n",
        "        prompt = self.prompts[idx] # 獲取與給定索引對應的提示\n",
        "\n",
        "        # Apply transformation if specified\n",
        "        if self.transform: # 如果指定瞭轉換\n",
        "            image = self.transform(image) # 應用轉換\n",
        "\n",
        "        # Return the transformed image and the prompt\n",
        "        return image, prompt # 返回轉換後的圖像和提示\n",
        "\n",
        "# Define a set of transformations to be applied to the data\n",
        "transform = transforms.Compose([ # 定義一組轉換，將被應用於資料\n",
        "    transforms.ToTensor(), # Convert PIL Image or numpy.ndarray to tensor, 將 PIL 圖像或 numpy.ndarray 轉換為張量\n",
        "    transforms.Normalize((0.5,), (0.5,)) # Normalize image with mean and standard deviation, 正規化圖像，使用均值和標準差\n",
        "])\n",
        "\n",
        "# Load the dataset using the defined transform\n",
        "dataset = TextToVideoDataset(root_dir='training_dataset', transform=transform) # 加載資料集，使用定義的轉換\n",
        "# Create a dataloader to iterate over the dataset, 創建一個資料加載器，用於迭代資料集\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True) # 創建一個資料加載器，用於迭代資料集"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvaajlG2j8s9"
      },
      "source": [
        "## Implementing GAN Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKgbkSNaj8s9"
      },
      "outputs": [],
      "source": [
        "# Define a class for text embedding\n",
        "class TextEmbedding(nn.Module):\n",
        "    # Constructor method with vocab_size and embed_size parameters\n",
        "    def __init__(self, vocab_size, embed_size):\n",
        "        # Call the superclass constructor\n",
        "        super(TextEmbedding, self).__init__()\n",
        "        # Initialize embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size) # 初始化嵌入層\n",
        "\n",
        "    # Define the forward pass method\n",
        "    def forward(self, x):\n",
        "        # Return embedded representation of input\n",
        "        return self.embedding(x) # 返回嵌入表示\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, text_embed_size):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        # Fully connected layer that takes noise and text embedding as input, 全連接層，接受噪聲和文本嵌入作為輸入\n",
        "        self.fc1 = nn.Linear(100 + text_embed_size, 256 * 8 * 8) # 全連接層，接受噪聲和文本嵌入作為輸入, 輸出大小為 256 * 8 * 8 (256 個通道，8 行，8 列)\n",
        "\n",
        "        # Transposed convolutional layers to upsample the input, 轉置卷積層，用於上採樣輸入\n",
        "        self.deconv1 = nn.ConvTranspose2d(256, 128, 4, 2, 1) # 轉置卷積層，用於上採樣輸入, 輸入大小為 256 個通道，輸出大小為 128 個通道，卷積核大小為 4x4，步幅為 2，填充為 1\n",
        "        self.deconv2 = nn.ConvTranspose2d(128, 64, 4, 2, 1) # 轉置卷積層，用於上採樣輸入, 輸入大小為 128 個通道，輸出大小為 64 個通道，卷積核大小為 4x4，步幅為 2，填充為 1\n",
        "        self.deconv3 = nn.ConvTranspose2d(64, 3, 4, 2, 1)  # Output has 3 channels for RGB images, 輸出有 3 個通道，用於 RGB 圖像, 輸入大小為 64 個通道，輸出大小為 3 個通道，卷積核大小為 4x4，步幅為 2，填充為 1\n",
        "\n",
        "        # Activation functions\n",
        "        self.relu = nn.ReLU(True)  # ReLU activation function, 激活函數\n",
        "        self.tanh = nn.Tanh()       # Tanh activation function for final output, 最終輸出的激活函數\n",
        "\n",
        "    def forward(self, noise, text_embed):\n",
        "        # Concatenate noise and text embedding along the channel dimension, 沿通道維度連接噪聲和文本嵌入\n",
        "        x = torch.cat((noise, text_embed), dim=1) # 沿通道維度連接噪聲和文本嵌入\n",
        "\n",
        "        # Fully connected layer followed by reshaping to 4D tensor, 全連接層，然後重塑為 4D 張量\n",
        "        x = self.fc1(x).view(-1, 256, 8, 8) # 全連接層，然後重塑為 4D 張量, 輸出大小為 256 * 8 * 8 (256 個通道，8 行，8 列), -1 表示自動計算批量大小\n",
        "\n",
        "        # Upsampling through transposed convolution layers with ReLU activation\n",
        "        x = self.relu(self.deconv1(x)) # 轉置卷積層，用於上採樣輸入, 輸入大小為 256 個通道，輸出大小為 128 個通道，卷積核大小為 4x4，步幅為 2，填充為 1\n",
        "        x = self.relu(self.deconv2(x)) # 轉置卷積層，用於上採樣輸入, 輸入大小為 128 個通道，輸出大小為 64 個通道，卷積核大小為 4x4，步幅為 2，填充為 1\n",
        "\n",
        "        # Final layer with Tanh activation to ensure output values are between -1 and 1 (for images)\n",
        "        x = self.tanh(self.deconv3(x)) # 轉置卷積層，用於上採樣輸入, 輸入大小為 64 個通道，輸出大小為 3 個通道，卷積核大小為 4x4，步幅為 2，填充為 1\n",
        "\n",
        "        return x\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        # Convolutional layers to process input images\n",
        "        self.conv1 = nn.Conv2d(3, 64, 4, 2, 1)   # 3 input channels (RGB), 64 output channels, kernel size 4x4, stride 2, padding 1, 輸入大小為 3 個通道，輸出大小為 64 個通道，卷積核大小為 4x4，步幅為 2，填充為 1\n",
        "        self.conv2 = nn.Conv2d(64, 128, 4, 2, 1) # 64 input channels, 128 output channels, kernel size 4x4, stride 2, padding 1, 輸入大小為 64 個通道，輸出大小為 128 個通道，卷積核大小為 4x4，步幅為 2，填充為 1\n",
        "        self.conv3 = nn.Conv2d(128, 256, 4, 2, 1) # 128 input channels, 256 output channels, kernel size 4x4, stride 2, padding 1, 輸入大小為 128 個通道，輸出大小為 256 個通道，卷積核大小為 4x4，步幅為 2，填充為 1\n",
        "\n",
        "        # Fully connected layer for classification\n",
        "        self.fc1 = nn.Linear(256 * 8 * 8, 1)  # Input size 256x8x8 (output size of last convolution), output size 1 (binary classification), 輸入大小為 256 * 8 * 8 (最後一個卷積層的輸出大小)，輸出大小為 1 (二元分類)\n",
        "\n",
        "        # Activation functions\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2, inplace=True)  # Leaky ReLU activation with negative slope 0.2, 帶有負斜率的 Leaky ReLU 激活函數\n",
        "        self.sigmoid = nn.Sigmoid()  # Sigmoid activation for final output (probability), 最終輸出的激活函數\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Pass input through convolutional layers with LeakyReLU activation\n",
        "        x = self.leaky_relu(self.conv1(input)) # 卷積層，用於處理輸入圖像, 輸入大小為 3 個通道，輸出大小為 64 個通道，卷積核大小為 4x4，步幅為 2，填充為 1\n",
        "        x = self.leaky_relu(self.conv2(x)) # 卷積層，用於處理輸入圖像, 輸入大小為 64 個通道，輸出大小為 128 個通道，卷積核大小為 4x4，步幅為 2，填充為 1\n",
        "        x = self.leaky_relu(self.conv3(x)) # 卷積層，用於處理輸入圖像, 輸入大小為 128 個通道，輸出大小為 256 個通道，卷積核大小為 4x4，步幅為 2，填充為 1\n",
        "\n",
        "        # Flatten the output of convolutional layers\n",
        "        x = x.view(-1, 256 * 8 * 8) # 重塑輸出，將 2D 卷積層的輸出重塑為 1D 向量\n",
        "\n",
        "        # Pass through fully connected layer with Sigmoid activation for binary classification\n",
        "        x = self.sigmoid(self.fc1(x)) # 全連接層，用於分類, 輸入大小為 256 * 8 * 8 (最後一個卷積層的輸出大小)，輸出大小為 1 (二元分類)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfCQkirWj8s9"
      },
      "source": [
        "## Coding Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9Zrz1szj8s9"
      },
      "outputs": [],
      "source": [
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # 檢查是否有 GPU，如果有則使用 GPU，否則使用 CPU\n",
        "\n",
        "# Create a simple vocabulary for text prompts, 創建一個簡單的文本提示詞典\n",
        "all_prompts = [prompt for prompt, _, _ in prompts_and_movements]  # Extract all prompts from prompts_and_movements list, 從 prompts_and_movements 列表中提取所有提示\n",
        "vocab = {word: idx for idx, word in enumerate(set(\" \".join(all_prompts).split()))}  # Create a vocabulary dictionary where each unique word is assigned an index\n",
        "vocab_size = len(vocab)  # Size of the vocabulary, 詞典的大小\n",
        "embed_size = 10  # Size of the text embedding vector, 文本嵌入向量的大小\n",
        "\n",
        "def encode_text(prompt): # 編碼給定的提示詞為索引的張量\n",
        "    # Encode a given prompt into a tensor of indices using the vocabulary\n",
        "    return torch.tensor([vocab[word] for word in prompt.split()]) # 將提示詞拆分為單詞，並將每個單詞轉換為索引，然後返回一個張量\n",
        "\n",
        "# Initialize models, loss function, and optimizers\n",
        "text_embedding = TextEmbedding(vocab_size, embed_size).to(device)  # Initialize TextEmbedding model with vocab_size and embed_size, 初始化文本嵌入模型，使用詞典大小和文本嵌入向量大小\n",
        "netG = Generator(embed_size).to(device)  # Initialize Generator model with embed_size, 初始化生成器模型，使用文本嵌入向量大小\n",
        "netD = Discriminator().to(device)  # Initialize Discriminator model, 初始化判別器模型\n",
        "criterion = nn.BCELoss().to(device)  # Binary Cross Entropy loss function, 二元交叉熵損失函數\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))  # Adam optimizer for Discriminator, 使用 Adam 優化器，學習率為 0.0002，beta1 為 0.5，beta2 為 0.999\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))  # Adam optimizer for Generator, 使用 Adam 優化器，學習率為 0.0002，beta1 為 0.5，beta2 為 0.999"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2tChYiEj8s9"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlSboIs5j8s9"
      },
      "outputs": [],
      "source": [
        "# Number of epochs\n",
        "num_epochs = 13 # 迭代次數\n",
        "\n",
        "# Iterate over each epoch\n",
        "for epoch in range(num_epochs): # 遍歷每個 epoch\n",
        "    # Iterate over each batch of data\n",
        "    for i, (data, prompts) in enumerate(dataloader): # 遍歷每個 batch\n",
        "        # Move real data to devicew\n",
        "        real_data = data.to(device) # 將實際數據移動到設備\n",
        "\n",
        "        # Convert prompts to list\n",
        "        prompts = [prompt for prompt in prompts] # 將提示轉換為列表\n",
        "\n",
        "        # Update Discriminator\n",
        "        netD.zero_grad()  # Zero the gradients of the Discriminator, 清零判別器的梯度\n",
        "        batch_size = real_data.size(0)  # Get the batch size, 獲取批量大小\n",
        "        labels = torch.ones(batch_size, 1).to(device)  # Create labels for real data (ones), 創建實際數據的標籤 (ones)\n",
        "        output = netD(real_data)  # Forward pass real data through Discriminator, 前向傳遞實際數據通過判別器\n",
        "        lossD_real = criterion(output, labels)  # Calculate loss on real data, 計算實際數據的損失\n",
        "        lossD_real.backward()  # Backward pass to calculate gradients, 反向傳遞計算梯度\n",
        "\n",
        "        # Generate fake data\n",
        "        noise = torch.randn(batch_size, 100).to(device)  # Generate random noise, 生成隨機噪聲\n",
        "        text_embeds = torch.stack([text_embedding(encode_text(prompt).to(device)).mean(dim=0) for prompt in prompts])  # Encode prompts into text embeddings, 將提示編碼為文本嵌入\n",
        "        fake_data = netG(noise, text_embeds)  # Generate fake data from noise and text embeddings, 從噪聲和文本嵌入生成假數據\n",
        "        labels = torch.zeros(batch_size, 1).to(device)  # Create labels for fake data (zeros), 創建假數據的標籤 (zeros) \n",
        "        output = netD(fake_data.detach())  # Forward pass fake data through Discriminator (detach to avoid gradients flowing back to Generator), 前向傳遞假數據通過判別器 (detach 避免梯度流回生成器)\n",
        "        lossD_fake = criterion(output, labels)  # Calculate loss on fake data, 計算假數據的損失\n",
        "        lossD_fake.backward()  # Backward pass to calculate gradients, 反向傳遞計算梯度\n",
        "        optimizerD.step()  # Update Discriminator parameters, 更新判別器參數\n",
        "\n",
        "        # Update Generator\n",
        "        netG.zero_grad()  # Zero the gradients of the Generator, 清零生成器的梯度\n",
        "        labels = torch.ones(batch_size, 1).to(device)  # Create labels for fake data (ones) to fool Discriminator, 創建假數據的標籤 (ones) 以欺騙判別器\n",
        "        output = netD(fake_data)  # Forward pass fake data (now updated) through Discriminator, 前向傳遞假數據 (現在更新) 通過判別器\n",
        "        lossG = criterion(output, labels)  # Calculate loss for Generator based on Discriminator's response, 計算生成器的損失，基於判別器的響應\n",
        "        lossG.backward()  # Backward pass to calculate gradients, 反向傳遞計算梯度 \n",
        "        optimizerG.step()  # Update Generator parameters, 更新生成器參數\n",
        "\n",
        "    # Print epoch information\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Loss D: {lossD_real + lossD_fake}, Loss G: {lossG}\") # 打印每個 epoch 的損失"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veLNs63Xj8s-"
      },
      "source": [
        "## Saving the Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IixqmS-kj8s-"
      },
      "outputs": [],
      "source": [
        "# Save the Generator model's state dictionary to a file named 'generator.pth'\n",
        "torch.save(netG.state_dict(), 'generator.pth') # 保存生成器模型\n",
        "\n",
        "# Save the Discriminator model's state dictionary to a file named 'discriminator.pth'\n",
        "torch.save(netD.state_dict(), 'discriminator.pth') # 保存判別器模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNlKX9P0j8s-"
      },
      "source": [
        "## Generating AI Video\n",
        "\n",
        "As we discussed, our approach to test our model on unseen data is comparable to the example where our training data involves dogs fetching balls and cats chasing mice. Therefore, our test prompt could involve scenarios like a cat fetching a ball or a dog chasing a mouse.\n",
        "In our specific case, the motion where the circle moves up and then to the right is not present in our training data, so the model is unfamiliar with this specific motion. However, it has been trained on other motions. We can use this motion as a prompt to test our trained model and observe its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RALjTc8Tj8s-"
      },
      "outputs": [],
      "source": [
        "# Inference function to generate a video based on a given text prompt \n",
        "def generate_video(text_prompt, num_frames=10): # 生成視頻的函數，給定文本提示和幀數\n",
        "    # Create a directory for the generated video frames based on the text prompt\n",
        "    os.makedirs(f'generated_video_{text_prompt.replace(\" \", \"_\")}', exist_ok=True) # 創建一個目錄，用於保存生成的視頻幀\n",
        "\n",
        "    # Encode the text prompt into a text embedding tensor\n",
        "    text_embed = text_embedding(encode_text(text_prompt).to(device)).mean(dim=0).unsqueeze(0) # 將文本提示編碼為文本嵌入\n",
        "\n",
        "    # Generate frames for the video\n",
        "    for frame_num in range(num_frames): # 遍歷每個幀\n",
        "        # Generate random noise\n",
        "        noise = torch.randn(1, 100).to(device) # 生成隨機噪聲\n",
        "\n",
        "        # Generate a fake frame using the Generator network\n",
        "        with torch.no_grad(): # 禁用梯度計算\n",
        "            fake_frame = netG(noise, text_embed) # 使用生成器網絡生成假幀\n",
        "\n",
        "        # Save the generated fake frame as an image file\n",
        "        save_image(fake_frame, f'generated_video_{text_prompt.replace(\" \", \"_\")}/frame_{frame_num}.png') # 保存生成的假幀為圖像文件\n",
        "\n",
        "# usage of the generate_video function with a specific text prompt 使用生成視頻的函數，給定文本提示 \n",
        "generate_video('circle moving up-right') # 使用生成視頻的函數，給定文本提示"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iymU6CVj8s-"
      },
      "source": [
        "When we run the above code, it will generate a directory containing all the frames of our generated video. We need to use a bit of code to merge all these frames into a single short video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gl9z6qoNj8s-"
      },
      "outputs": [],
      "source": [
        "# Define the path to your folder containing the PNG frames, 定義包含 PNG 幀的文件夾的路徑\n",
        "folder_path = 'generated_video_circle_moving_up-right' \n",
        "\n",
        "# Get the list of all PNG files in the folder\n",
        "image_files = [f for f in os.listdir(folder_path) if f.endswith('.png')] # 獲取文件夾中的所有 PNG 文件\n",
        "\n",
        "# Sort the images by name (assuming they are numbered sequentially)\n",
        "image_files.sort() # 按名稱排序圖像\n",
        "\n",
        "# Create a list to store the frames\n",
        "frames = [] # 創建一個列表，用於存儲幀\n",
        "\n",
        "# Read each image and append it to the frames list\n",
        "for image_file in image_files: # 遍歷每個圖像文件\n",
        "  image_path = os.path.join(folder_path, image_file) # 獲取圖像文件的路徑\n",
        "  frame = cv2.imread(image_path) # 讀取圖像文件\n",
        "  frames.append(frame) # 將幀添加到列表中\n",
        "\n",
        "# Convert the frames list to a numpy array for easier processing\n",
        "frames = np.array(frames) # 將幀列表轉換為 numpy 數組，便於處理\n",
        "\n",
        "# Define the frame rate (frames per second)\n",
        "fps = 10 # 定義幀率 (幀每秒)\n",
        "\n",
        "# Create a video writer object\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID') # 創建一個視頻寫入對象\n",
        "out = cv2.VideoWriter('generated_video.avi', fourcc, fps, (frames[0].shape[1], frames[0].shape[0])) # 創建一個視頻寫入對象\n",
        "\n",
        "# Write each frame to the video\n",
        "for frame in frames: # 遍歷每個幀\n",
        "  out.write(frame) # 將幀寫入視頻\n",
        "\n",
        "# Release the video writer\n",
        "out.release() # 釋放視頻寫入對象"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sqy1GxSRj8s-"
      },
      "source": [
        "Make sure the folder path points to where your newly generated video exists. After running this code, your AI video will have been successfully created. Let's see what it looks like."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRLTU_onj8s-"
      },
      "source": [
        "## What's Missing?\n",
        "I've tested various aspects of this architecture, and found that the training data is the key. By including more motions and shapes in the dataset, you can increase variability and improve the model's performance. Since the data is generated through code, generating more varied data won't take much time; instead, you can focus on refining the logic.\n",
        "Furthermore, the GAN architecture discussed in this blog is relatively straightforward. You can make it more complex by integrating advanced techniques or using a language model embedding (LLM) instead of a basic neural network embedding. Additionally, tuning parameters such as embedding size and others can significantly impact the model's effectiveness."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
