import torch
import json
import os
import getpass  # æ·»åŠ å®‰å…¨è¼¸å…¥æ¨¡çµ„
from huggingface_hub import HfApi, Repository, create_repo
from encoder_decoder_transformer import Transformer
import shutil

def create_model_card(model_name, vocab_size, d_model, n_heads, n_layers, dataset_info):
    """å‰µå»ºæ¨¡å‹å¡ç‰‡"""
    model_card = f"""---
language: en
license: mit
tags:
- transformer
- text-generation
- shakespeare
- pytorch
datasets:
- custom
widget:
- text: "To be or not to be"
  example_title: "Shakespeare Style"
---

# {model_name}

## æ¨¡å‹æè¿°

é€™æ˜¯ä¸€å€‹åŸºæ–¼ Transformer æ¶æ§‹çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œä½¿ç”¨èå£«æ¯”äºæ–‡æœ¬é€²è¡Œè¨“ç·´ã€‚

## æ¨¡å‹æ¶æ§‹

- **æ¨¡å‹é¡å‹**: Encoder-Decoder Transformer
- **è©å½™è¡¨å¤§å°**: {vocab_size:,}
- **æ¨¡å‹ç¶­åº¦**: {d_model}
- **æ³¨æ„åŠ›é ­æ•¸**: {n_heads}
- **ç·¨ç¢¼å™¨å±¤æ•¸**: {n_layers}
- **è§£ç¢¼å™¨å±¤æ•¸**: {n_layers}

## è¨“ç·´æ•¸æ“š

{dataset_info}

## ä½¿ç”¨æ–¹æ³•

```python
import torch
from transformers import AutoModel

# è¼‰å…¥æ¨¡å‹
model = AutoModel.from_pretrained("{model_name}")

# é€²è¡Œæ¨ç†ï¼ˆéœ€è¦è‡ªå®šç¾©æ¨ç†ä»£ç¢¼ï¼‰
# è©³è¦‹å€‰åº«ä¸­çš„ä½¿ç”¨ç¤ºä¾‹
```

## è¨“ç·´ç´°ç¯€

- **å„ªåŒ–å™¨**: Adam
- **å­¸ç¿’ç‡èª¿åº¦**: StepLR
- **è¨“ç·´æ¡†æ¶**: PyTorch
- **ä»»å‹™é¡å‹**: æ–‡æœ¬ç”Ÿæˆï¼ˆå¾©è¿°ä»»å‹™ï¼‰

## é™åˆ¶å’Œåè¦‹

- æ¨¡å‹åœ¨èå£«æ¯”äºæ–‡æœ¬ä¸Šè¨“ç·´ï¼Œå¯èƒ½æœƒåæ˜ è©²æ™‚ä»£çš„èªè¨€ç‰¹é»
- é©ç”¨æ–¼å¤å…¸è‹±æ–‡æ–‡æœ¬ç”Ÿæˆï¼Œç¾ä»£æ–‡æœ¬æ•ˆæœå¯èƒ½æœ‰é™

```
"""
    return model_card

def create_config_json(config):
    """å‰µå»ºé…ç½®æ–‡ä»¶"""
    hf_config = {
        "architectures": ["Transformer"],
        "model_type": "transformer",
        "vocab_size": config["src_vocab_size"],
        "d_model": config["d_model"],
        "n_heads": config["n_heads"],
        "n_encoder_layers": config["n_encoder_layers"],
        "n_decoder_layers": config["n_decoder_layers"],
        "d_ff": config["d_ff"],
        "dropout": config["dropout"],
        "pad_token_id": config["pad_idx"],
        "bos_token_id": 2,  # å‡è¨­ BOS token ID
        "eos_token_id": 3,  # å‡è¨­ EOS token ID
        "max_position_embeddings": 512,
        "torch_dtype": "float32",
        "transformers_version": "4.0.0"
    }
    return hf_config

def create_training_args_json():
    """å‰µå»ºè¨“ç·´åƒæ•¸æ–‡ä»¶"""
    training_args = {
        "output_dir": "./results",
        "num_train_epochs": 10,
        "per_device_train_batch_size": 16,
        "gradient_accumulation_steps": 1,
        "learning_rate": 1e-4,
        "weight_decay": 0.01,
        "logging_steps": 100,
        "save_steps": 1000,
        "eval_steps": 500,
        "warmup_steps": 500,
        "max_grad_norm": 1.0,
        "fp16": False,
        "dataloader_num_workers": 4,
        "load_best_model_at_end": True,
        "metric_for_best_model": "loss",
        "greater_is_better": False,
        "report_to": [],
        "seed": 42
    }
    return training_args

def create_usage_example():
    """å‰µå»ºä½¿ç”¨ç¤ºä¾‹æ–‡ä»¶"""
    example_code = '''
import torch
from encoder_decoder_transformer import Transformer
import json

# è¼‰å…¥æ¨¡å‹å’Œé…ç½®
def load_model(model_path):
    checkpoint = torch.load(f"{model_path}/pytorch_model.bin", map_location="cpu")
    
    with open(f"{model_path}/config.json", "r") as f:
        config = json.load(f)
    
    # é‡å»ºæ¨¡å‹
    model = Transformer(
        src_vocab_size=config["vocab_size"],
        tgt_vocab_size=config["vocab_size"],
        d_model=config["d_model"],
        n_heads=config["n_heads"],
        n_encoder_layers=config["n_encoder_layers"],
        n_decoder_layers=config["n_decoder_layers"],
        d_ff=config["d_ff"],
        dropout=config["dropout"],
        pad_idx=config["pad_token_id"]
    )
    
    # è¼‰å…¥æ¬Šé‡
    model.load_state_dict(checkpoint["model_state_dict"])
    model.eval()
    
    return model, checkpoint["vocab"], checkpoint["idx_to_vocab"]

# ä½¿ç”¨ç¤ºä¾‹
def generate_text(model, vocab, idx_to_vocab, input_text, max_length=50):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    
    # ç°¡å–®çš„tokenization
    words = input_text.lower().split()
    tokens = [vocab.get("<BOS>", 2)] + [vocab.get(word, vocab.get("<UNK>", 1)) for word in words] + [vocab.get("<EOS>", 3)]
    
    # å¡«å……
    max_len = 64
    if len(tokens) < max_len:
        tokens += [vocab.get("<PAD>", 0)] * (max_len - len(tokens))
    else:
        tokens = tokens[:max_len]
    
    src = torch.tensor(tokens).unsqueeze(0).to(device)
    
    # ç”Ÿæˆ
    with torch.no_grad():
        generated = model.generate(src, max_len=max_length, start_token=2, end_token=3)
    
    # è½‰æ›ç‚ºæ–‡æœ¬
    words = []
    for token in generated[0]:
        word = idx_to_vocab.get(token.item(), "<UNK>")
        if word in ["<PAD>", "<BOS>", "<EOS>"]:
            if word == "<EOS>":
                break
            continue
        words.append(word)
    
    return " ".join(words)

# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    model, vocab, idx_to_vocab = load_model("./")
    
    input_texts = [
        "To be or not to be",
        "What is your name",
        "The king is dead"
    ]
    
    for text in input_texts:
        generated = generate_text(model, vocab, idx_to_vocab, text)
        print(f"è¼¸å…¥: {text}")
        print(f"ç”Ÿæˆ: {generated}")
        print("-" * 50)
'''
    return example_code

def push_to_huggingface(
    model_path="transformer_model.pth",
    repo_name="shakespeare-transformer",
    username=None,
    token=None,
    private=False,
    commit_message="Add Shakespeare Transformer model"
):
    """
    å°‡æ¨¡å‹æ¨é€åˆ° Hugging Face Hub
    
    åƒæ•¸:
    - model_path: æœ¬åœ°æ¨¡å‹æ–‡ä»¶è·¯å¾‘
    - repo_name: å€‰åº«åç¨±
    - username: Hugging Face ç”¨æˆ¶å
    - token: Hugging Face token
    - private: æ˜¯å¦ç‚ºç§æœ‰å€‰åº«
    - commit_message: æäº¤ä¿¡æ¯
    """
    
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
    
    # å®‰å…¨åœ°ç²å–ç”¨æˆ¶åå’Œtoken
    if username is None:
        username = input("è«‹è¼¸å…¥æ‚¨çš„ Hugging Face ç”¨æˆ¶å: ")
    
    if token is None:
        print("è«‹è¼¸å…¥æ‚¨çš„ Hugging Face tokenï¼ˆè¼¸å…¥æ™‚ä¸æœƒé¡¯ç¤ºï¼‰:")
        print("ğŸ’¡ æç¤º: æ‚¨å¯ä»¥åœ¨ https://huggingface.co/settings/tokens ç²å– token")
        token = getpass.getpass("Token: ")
        
        # é©—è­‰ token ä¸ç‚ºç©º
        while not token.strip():
            print("âŒ Token ä¸èƒ½ç‚ºç©ºï¼Œè«‹é‡æ–°è¼¸å…¥:")
            token = getpass.getpass("Token: ")
    
    # å®Œæ•´çš„å€‰åº«åç¨±
    full_repo_name = f"{username}/{repo_name}"
    
    try:
        # åˆå§‹åŒ– API
        api = HfApi()
        
        # è¼‰å…¥æ¨¡å‹æª¢æŸ¥é»
        print("è¼‰å…¥æ¨¡å‹...")
        checkpoint = torch.load(model_path, map_location="cpu")
        config = checkpoint["config"]
        vocab = checkpoint["vocab"]
        idx_to_vocab = checkpoint["idx_to_vocab"]
        
        # å‰µå»ºå€‰åº«
        print(f"å‰µå»ºå€‰åº«: {full_repo_name}")
        try:
            create_repo(
                repo_id=full_repo_name,
                token=token,
                private=private,
                exist_ok=True
            )
        except Exception as e:
            print(f"å€‰åº«å‰µå»ºè­¦å‘Š: {e}")
        
        # å‰µå»ºè‡¨æ™‚ç›®éŒ„
        temp_dir = f"./temp_{repo_name}"
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
        os.makedirs(temp_dir)
        
        try:
            # å…‹éš†å€‰åº«
            print("å…‹éš†å€‰åº«...")
            repo = Repository(
                local_dir=temp_dir,
                clone_from=full_repo_name,
                use_auth_token=token
            )
            
            # ä¿å­˜ PyTorch æ¨¡å‹æ¬Šé‡
            print("ä¿å­˜æ¨¡å‹æ–‡ä»¶...")
            torch.save({
                "model_state_dict": checkpoint["model_state_dict"],
                "vocab": vocab,
                "idx_to_vocab": idx_to_vocab
            }, os.path.join(temp_dir, "pytorch_model.bin"))
            
            # å‰µå»ºé…ç½®æ–‡ä»¶
            print("å‰µå»ºé…ç½®æ–‡ä»¶...")
            hf_config = create_config_json(config)
            with open(os.path.join(temp_dir, "config.json"), "w", encoding="utf-8") as f:
                json.dump(hf_config, f, indent=2, ensure_ascii=False)
            
            # ä¿å­˜è©å½™è¡¨
            print("ä¿å­˜è©å½™è¡¨...")
            with open(os.path.join(temp_dir, "vocab.json"), "w", encoding="utf-8") as f:
                json.dump(vocab, f, indent=2, ensure_ascii=False)
            
            with open(os.path.join(temp_dir, "idx_to_vocab.json"), "w", encoding="utf-8") as f:
                json.dump(idx_to_vocab, f, indent=2, ensure_ascii=False)
            
            # å‰µå»ºè¨“ç·´åƒæ•¸æ–‡ä»¶
            training_args = create_training_args_json()
            with open(os.path.join(temp_dir, "training_args.json"), "w", encoding="utf-8") as f:
                json.dump(training_args, f, indent=2)
            
            # å‰µå»ºæ¨¡å‹å¡ç‰‡
            print("å‰µå»ºæ¨¡å‹å¡ç‰‡...")
            dataset_info = f"ä½¿ç”¨èå£«æ¯”äºæ–‡æœ¬é€²è¡Œè¨“ç·´ï¼ŒåŒ…å« {len(checkpoint.get('dataset_info', {}).get('sentences', []))} å€‹å¥å­"
            model_card = create_model_card(
                full_repo_name, 
                config["src_vocab_size"], 
                config["d_model"], 
                config["n_heads"], 
                config["n_encoder_layers"],
                dataset_info
            )
            with open(os.path.join(temp_dir, "README.md"), "w", encoding="utf-8") as f:
                f.write(model_card)
            
            # å‰µå»ºä½¿ç”¨ç¤ºä¾‹æ–‡ä»¶
            print("å‰µå»ºä½¿ç”¨ç¤ºä¾‹...")
            example_code = create_usage_example()
            with open(os.path.join(temp_dir, "usage_example.py"), "w", encoding="utf-8") as f:
                f.write(example_code)
            
            # è¤‡è£½æ¨¡å‹å®šç¾©æ–‡ä»¶
            if os.path.exists("encoder_decoder_transformer.py"):
                shutil.copy("encoder_decoder_transformer.py", temp_dir)
                print("è¤‡è£½æ¨¡å‹å®šç¾©æ–‡ä»¶...")
            
            # å‰µå»ºéœ€æ±‚æ–‡ä»¶
            requirements = """torch>=2.0.0
transformers>=4.21.0
numpy>=1.21.0
huggingface_hub>=0.16.0
"""
            with open(os.path.join(temp_dir, "requirements.txt"), "w") as f:
                f.write(requirements)
            
            # æäº¤å’Œæ¨é€
            print("æäº¤æ›´æ”¹...")
            repo.git_add()
            repo.git_commit(commit_message)
            repo.git_push()
            
            print(f"âœ… æ¨¡å‹æˆåŠŸæ¨é€åˆ° Hugging Face Hub!")
            print(f"ğŸ”— å€‰åº«é€£çµ: https://huggingface.co/{full_repo_name}")
            print(f"ğŸ“¦ æ¨¡å‹å¯ä»¥é€šéä»¥ä¸‹æ–¹å¼è¼‰å…¥:")
            print(f"   from huggingface_hub import hf_hub_download")
            print(f"   hf_hub_download('{full_repo_name}', 'pytorch_model.bin')")
            
        finally:
            # æ¸…ç†è‡¨æ™‚ç›®éŒ„
            if os.path.exists(temp_dir):
                shutil.rmtree(temp_dir)
                print("æ¸…ç†è‡¨æ™‚æ–‡ä»¶...")
                
    except Exception as e:
        print(f"âŒ æ¨é€å¤±æ•—: {str(e)}")
        print("è«‹æª¢æŸ¥:")
        print("1. Hugging Face token æ˜¯å¦æ­£ç¢º")
        print("2. ç”¨æˆ¶åæ˜¯å¦æ­£ç¢º")
        print("3. ç¶²çµ¡é€£æ¥æ˜¯å¦æ­£å¸¸")
        print("4. æ˜¯å¦å·²å®‰è£ huggingface_hub: pip install huggingface_hub")
        raise

def get_user_credentials():
    """å®‰å…¨åœ°ç²å–ç”¨æˆ¶æ†‘è­‰"""
    print("ğŸ¤— Hugging Face ç™»éŒ„")
    print("-" * 30)
    
    username = input("ç”¨æˆ¶å (ewdlop): ").strip()
    if not username:
        username = "ewdlop"  # é»˜èªç”¨æˆ¶å
    
    print("\nğŸ’¡ ç²å– Token çš„æ­¥é©Ÿ:")
    print("1. è¨ªå• https://huggingface.co/settings/tokens")
    print("2. é»æ“Š 'New token'")
    print("3. é¸æ“‡ 'Write' æ¬Šé™")
    print("4. è¤‡è£½ç”Ÿæˆçš„ token")
    print()
    
    token = getpass.getpass("è«‹è¼¸å…¥æ‚¨çš„ Hugging Face token (è¼¸å…¥æ™‚ä¸æœƒé¡¯ç¤º): ")
    
    while not token.strip():
        print("âŒ Token ä¸èƒ½ç‚ºç©ºï¼Œè«‹é‡æ–°è¼¸å…¥:")
        token = getpass.getpass("Token: ")
    
    # ç¢ºèªä¿¡æ¯
    print(f"\nâœ… ç”¨æˆ¶å: {username}")
    print("âœ… Token: [å·²è¨­ç½®]")
    
    confirm = input("\nç¢ºèªä¿¡æ¯æ­£ç¢ºå—? (Y/n): ").lower().strip()
    if confirm in ['n', 'no']:
        print("é‡æ–°è¼¸å…¥...")
        return get_user_credentials()
    
    return username, token

def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ¨é€ Transformer æ¨¡å‹åˆ° Hugging Face Hub")
    parser.add_argument("--model_path", default="transformer_model.pth", 
                       help="æ¨¡å‹æ–‡ä»¶è·¯å¾‘")
    parser.add_argument("--repo_name", default="shakespeare-transformer", 
                       help="å€‰åº«åç¨±")
    parser.add_argument("--username", default="ewdlop", help="Hugging Face ç”¨æˆ¶å")
    parser.add_argument("--token", help="Hugging Face token (ä¸å»ºè­°åœ¨å‘½ä»¤è¡Œä¸­ä½¿ç”¨)")
    parser.add_argument("--private", action="store_true", 
                       help="å‰µå»ºç§æœ‰å€‰åº«")
    parser.add_argument("--commit_message", 
                       default="Add Shakespeare Transformer model",
                       help="æäº¤ä¿¡æ¯")
    parser.add_argument("--interactive", action="store_true",
                       help="äº¤äº’å¼è¼¸å…¥æ†‘è­‰")
    
    args = parser.parse_args()
    
    print("ğŸ¤— æº–å‚™æ¨é€æ¨¡å‹åˆ° Hugging Face Hub...")
    print(f"æ¨¡å‹æ–‡ä»¶: {args.model_path}")
    print(f"å€‰åº«åç¨±: {args.repo_name}")
    print(f"ç§æœ‰å€‰åº«: {args.private}")
    print()
    
    # ç²å–æ†‘è­‰
    username = args.username
    token = args.token
    
    # å¦‚æœä½¿ç”¨äº¤äº’æ¨¡å¼æˆ–æ²’æœ‰æä¾› token
    if args.interactive or not token:
        username, token = get_user_credentials()
    
    push_to_huggingface(
        model_path=args.model_path,
        repo_name=args.repo_name,
        username=username,
        token=token,
        private=args.private,
        commit_message=args.commit_message
    )

if __name__ == "__main__":
    main() 