#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Sprite Sheet ç”Ÿæˆå™¨ - ç„¡ xformers ç‰ˆæœ¬
ä½¿ç”¨ PyTorch åŸç”Ÿå„ªåŒ–æ›¿ä»£ xformers
"""

import torch
import torch.nn.functional as F
import argparse
from tqdm import tqdm
from typing import Tuple, Dict, Any
from PIL import Image

# å°å…¥ AI æ¨¡å‹
from diffusers import (
    StableDiffusionControlNetPipeline,
    ControlNetModel,
    DPMSolverMultistepScheduler
)
from controlnet_aux import OpenposeDetector

# å°å…¥å§¿å‹¢æ“ä½œæ¨¡çµ„
from pose_manipulation import PoseManipulator, AnimationType

class MemoryOptimizedSpriteGenerator:
    """è¨˜æ†¶é«”å„ªåŒ–çš„ Sprite Sheet ç”Ÿæˆå™¨ï¼ˆç„¡ xformersï¼‰"""
    
    def __init__(
        self,
        model_id: str = "runwayml/stable-diffusion-v1-5",
        controlnet_id: str = "lllyasviel/sd-controlnet-openpose",
        device: str = "auto",
        enable_cpu_offload: bool = True,
        enable_model_optimization: bool = True,
        low_vram_mode: bool = False
    ):
        self.device = self._setup_device(device)
        self.model_id = model_id
        self.controlnet_id = controlnet_id
        self.enable_cpu_offload = enable_cpu_offload
        self.enable_model_optimization = enable_model_optimization
        self.low_vram_mode = low_vram_mode
        
        print(f"ğŸš€ åˆå§‹åŒ– Sprite ç”Ÿæˆå™¨ï¼ˆç„¡ xformersï¼‰")
        print(f"ğŸ“± è¨­å‚™: {self.device}")
        print(f"ğŸ’¾ CPU å¸è¼‰: {enable_cpu_offload}")
        print(f"ğŸ”§ æ¨¡å‹å„ªåŒ–: {enable_model_optimization}")
        print(f"âš¡ ä½é¡¯å­˜æ¨¡å¼: {low_vram_mode}")
        
        self._initialize_models()
        self.pose_manipulator = PoseManipulator()
        
    def _setup_device(self, device: str) -> str:
        """è¨­ç½®è¨ˆç®—è¨­å‚™"""
        if device == "auto":
            if torch.cuda.is_available():
                device = "cuda"
                print(f"âœ… æ‰¾åˆ° CUDA è¨­å‚™: {torch.cuda.get_device_name(0)}")
                # é¡¯ç¤º GPU è¨˜æ†¶é«”ä¿¡æ¯
                total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
                print(f"ğŸ“Š GPU è¨˜æ†¶é«”: {total_memory:.1f} GB")
            else:
                device = "cpu"
                print("âš ï¸ æœªæ‰¾åˆ° CUDAï¼Œä½¿ç”¨ CPU")
        return device
    
    def _apply_memory_optimizations(self, pipe):
        """æ‡‰ç”¨è¨˜æ†¶é«”å„ªåŒ–æŠ€è¡“ï¼ˆæ›¿ä»£ xformersï¼‰"""
        if self.enable_model_optimization:
            print("ğŸ”§ æ‡‰ç”¨è¨˜æ†¶é«”å„ªåŒ–...")
            
            # 1. å•Ÿç”¨æ³¨æ„åŠ›åˆ‡ç‰‡
            pipe.enable_attention_slicing()
            print("âœ… æ³¨æ„åŠ›åˆ‡ç‰‡å·²å•Ÿç”¨")
            
            # 2. å•Ÿç”¨ VAE åˆ‡ç‰‡
            if hasattr(pipe, 'enable_vae_slicing'):
                pipe.enable_vae_slicing()
                print("âœ… VAE åˆ‡ç‰‡å·²å•Ÿç”¨")
            
            # 3. ä½¿ç”¨è¨˜æ†¶é«”é«˜æ•ˆçš„æ³¨æ„åŠ›
            if self.device == "cuda":
                # ä½¿ç”¨ PyTorch 2.0 çš„åŸç”Ÿ SDPAï¼ˆScaled Dot Product Attentionï¼‰
                if hasattr(torch.nn.functional, 'scaled_dot_product_attention'):
                    print("âœ… ä½¿ç”¨ PyTorch åŸç”Ÿ SDPA")
                else:
                    print("âš ï¸ PyTorch ç‰ˆæœ¬è¼ƒèˆŠï¼Œç„¡æ³•ä½¿ç”¨åŸç”Ÿ SDPA")
            
            # 4. ä½é¡¯å­˜æ¨¡å¼å„ªåŒ–
            if self.low_vram_mode:
                pipe.enable_sequential_cpu_offload()
                print("âœ… é †åº CPU å¸è¼‰å·²å•Ÿç”¨ï¼ˆä½é¡¯å­˜æ¨¡å¼ï¼‰")
            elif self.enable_cpu_offload and self.device == "cuda":
                pipe.enable_model_cpu_offload()
                print("âœ… æ¨¡å‹ CPU å¸è¼‰å·²å•Ÿç”¨")
            
            # 5. ç·¨è­¯æ¨¡å‹ï¼ˆPyTorch 2.0+ï¼‰
            if hasattr(torch, 'compile'):
                try:
                    pipe.unet = torch.compile(pipe.unet, mode="reduce-overhead")
                    print("âœ… UNet æ¨¡å‹å·²ç·¨è­¯å„ªåŒ–")
                except Exception as e:
                    print(f"âš ï¸ æ¨¡å‹ç·¨è­¯å¤±æ•—: {e}")
        
        return pipe
    
    def _initialize_models(self):
        """åˆå§‹åŒ– AI æ¨¡å‹"""
        print("ğŸ”„ è¼‰å…¥ ControlNet æ¨¡å‹...")
        self.controlnet = ControlNetModel.from_pretrained(
            self.controlnet_id,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32
        )
        
        print("ğŸ”„ è¼‰å…¥ Stable Diffusion ç®¡é“...")
        self.pipe = StableDiffusionControlNetPipeline.from_pretrained(
            self.model_id,
            controlnet=self.controlnet,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            safety_checker=None,
            requires_safety_checker=False
        )
        
        # è¨­ç½®æ›´é«˜æ•ˆçš„èª¿åº¦å™¨
        self.pipe.scheduler = DPMSolverMultistepScheduler.from_config(
            self.pipe.scheduler.config
        )
        
        # æ‡‰ç”¨è¨˜æ†¶é«”å„ªåŒ–
        self.pipe = self._apply_memory_optimizations(self.pipe)
        
        # ç§»å‹•åˆ°è¨­å‚™
        if not self.enable_cpu_offload:
            self.pipe = self.pipe.to(self.device)
        
        print("ğŸ”„ è¼‰å…¥å§¿å‹¢æª¢æ¸¬å™¨...")
        self.pose_detector = OpenposeDetector.from_pretrained("lllyasviel/Annotators")
        
        print("âœ… æ‰€æœ‰æ¨¡å‹è¼‰å…¥å®Œæˆ")
    
    def optimize_generation_params(self, vram_gb: float = None) -> Dict[str, Any]:
        """æ ¹æ“šå¯ç”¨ VRAM å„ªåŒ–ç”Ÿæˆåƒæ•¸"""
        if vram_gb is None and self.device == "cuda":
            vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
        
        if vram_gb is None or vram_gb >= 8:
            # é«˜ç«¯è¨­ç½®
            return {
                "num_inference_steps": 30,
                "guidance_scale": 7.5,
                "width": 512,
                "height": 512,
                "batch_size": 2
            }
        elif vram_gb >= 6:
            # ä¸­ç«¯è¨­ç½®
            return {
                "num_inference_steps": 25,
                "guidance_scale": 7.0,
                "width": 512,
                "height": 512,
                "batch_size": 1
            }
        else:
            # ä½ç«¯è¨­ç½®
            return {
                "num_inference_steps": 20,
                "guidance_scale": 6.5,
                "width": 384,
                "height": 384,
                "batch_size": 1
            }
    
    def generate_sprite_with_pose(
        self,
        prompt: str,
        pose_image: Image.Image,
        negative_prompt: str = None,
        **kwargs
    ) -> Image.Image:
        """ä½¿ç”¨å§¿å‹¢æ§åˆ¶ç”Ÿæˆå–®å€‹ç²¾éˆåœ–åƒ"""
        # ç²å–å„ªåŒ–åƒæ•¸
        default_params = self.optimize_generation_params()
        
        # åˆä½µç”¨æˆ¶åƒæ•¸
        params = {**default_params, **kwargs}
        
        # è¨­ç½®é è¨­è² é¢æç¤ºè©
        if negative_prompt is None:
            negative_prompt = (
                "blurry, low quality, distorted, deformed, "
                "extra limbs, bad anatomy, worst quality"
            )
        
        # æ¸…ç† GPU è¨˜æ†¶é«”
        if self.device == "cuda":
            torch.cuda.empty_cache()
        
        # ç”Ÿæˆåœ–åƒ
        with torch.inference_mode():
            try:
                result = self.pipe(
                    prompt=prompt,
                    image=pose_image,
                    negative_prompt=negative_prompt,
                    num_inference_steps=params["num_inference_steps"],
                    guidance_scale=params["guidance_scale"],
                    width=params["width"],
                    height=params["height"],
                    generator=torch.Generator(device=self.device).manual_seed(42)
                )
                
                return result.images[0]
                
            except torch.cuda.OutOfMemoryError:
                print("âš ï¸ GPU è¨˜æ†¶é«”ä¸è¶³ï¼Œåˆ‡æ›åˆ° CPU å¸è¼‰æ¨¡å¼")
                if hasattr(self.pipe, 'enable_sequential_cpu_offload'):
                    self.pipe.enable_sequential_cpu_offload()
                return self.generate_sprite_with_pose(
                    prompt, pose_image, negative_prompt, **kwargs
                )
    
    def create_sprite_sheet(
        self,
        character_prompt: str,
        animation_type: AnimationType = AnimationType.WALK,
        frames: int = 8,
        sprite_size: Tuple[int, int] = (64, 64),
        sheet_cols: int = 4,
        output_path: str = "sprite_sheet.png",
        style_prompt: str = "pixel art style",
        **kwargs
    ) -> Image.Image:
        """å‰µå»ºå®Œæ•´çš„ Sprite Sheet"""
        print(f"ğŸ¨ å‰µå»º Sprite Sheet: {animation_type.value}")
        print(f"ğŸ“ ç²¾éˆå°ºå¯¸: {sprite_size}")
        print(f"ğŸï¸ å¹€æ•¸: {frames}")
        
        # ç”Ÿæˆå§¿å‹¢åºåˆ—
        poses = self.pose_manipulator.generate_animation_sequence(
            animation_type, frames
        )
        
        # æº–å‚™å®Œæ•´æç¤ºè©
        full_prompt = f"{character_prompt}, {style_prompt}, clean background"
        
        # ç”Ÿæˆæ‰€æœ‰å¹€
        generated_frames = []
        for i, pose in enumerate(tqdm(poses, desc="ç”Ÿæˆå¹€")):
            # æ¸²æŸ“å§¿å‹¢
            pose_image = self.pose_manipulator.render_pose(
                pose, size=sprite_size, show_skeleton=False
            )
            
            # ç”Ÿæˆç²¾éˆåœ–åƒ
            sprite_image = self.generate_sprite_with_pose(
                full_prompt,
                pose_image,
                width=sprite_size[0],
                height=sprite_size[1],
                **kwargs
            )
            
            # èª¿æ•´å¤§å°ç¢ºä¿ä¸€è‡´æ€§
            sprite_image = sprite_image.resize(sprite_size, Image.Resampling.LANCZOS)
            generated_frames.append(sprite_image)
            
            # æ¸…ç†è¨˜æ†¶é«”
            if self.device == "cuda":
                torch.cuda.empty_cache()
        
        # å‰µå»º Sprite Sheet
        sheet_rows = (frames + sheet_cols - 1) // sheet_cols
        sheet_width = sheet_cols * sprite_size[0]
        sheet_height = sheet_rows * sprite_size[1]
        
        sprite_sheet = Image.new('RGBA', (sheet_width, sheet_height), (0, 0, 0, 0))
        
        for i, frame in enumerate(generated_frames):
            row = i // sheet_cols
            col = i % sheet_cols
            x = col * sprite_size[0]
            y = row * sprite_size[1]
            sprite_sheet.paste(frame, (x, y))
        
        # ä¿å­˜çµæœ
        sprite_sheet.save(output_path)
        print(f"âœ… Sprite Sheet å·²ä¿å­˜: {output_path}")
        
        return sprite_sheet

def main():
    """å‘½ä»¤è¡Œç•Œé¢"""
    parser = argparse.ArgumentParser(description="Sprite Sheet ç”Ÿæˆå™¨ï¼ˆç„¡ xformersï¼‰")
    parser.add_argument("--prompt", required=True, help="è§’è‰²æè¿°æç¤ºè©")
    parser.add_argument("--animation", default="walk", 
                       choices=["idle", "walk", "run", "jump", "attack"],
                       help="å‹•ç•«é¡å‹")
    parser.add_argument("--frames", type=int, default=8, help="å‹•ç•«å¹€æ•¸")
    parser.add_argument("--size", nargs=2, type=int, default=[64, 64], 
                       help="ç²¾éˆå°ºå¯¸ (å¯¬ é«˜)")
    parser.add_argument("--cols", type=int, default=4, help="Sprite Sheet åˆ—æ•¸")
    parser.add_argument("--output", default="sprite_sheet.png", help="è¼¸å‡ºæª”æ¡ˆè·¯å¾‘")
    parser.add_argument("--style", default="pixel art style", help="è—è¡“é¢¨æ ¼")
    parser.add_argument("--low-vram", action="store_true", help="å•Ÿç”¨ä½é¡¯å­˜æ¨¡å¼")
    parser.add_argument("--cpu-only", action="store_true", help="åƒ…ä½¿ç”¨ CPU")
    
    args = parser.parse_args()
    
    # è¨­ç½®è¨­å‚™
    device = "cpu" if args.cpu_only else "auto"
    
    # å‰µå»ºç”Ÿæˆå™¨
    generator = MemoryOptimizedSpriteGenerator(
        device=device,
        low_vram_mode=args.low_vram,
        enable_cpu_offload=not args.cpu_only
    )
    
    # è½‰æ›å‹•ç•«é¡å‹
    animation_type = AnimationType(args.animation.upper())
    
    # ç”Ÿæˆ Sprite Sheet
    try:
        sprite_sheet = generator.create_sprite_sheet(
            character_prompt=args.prompt,
            animation_type=animation_type,
            frames=args.frames,
            sprite_size=tuple(args.size),
            sheet_cols=args.cols,
            output_path=args.output,
            style_prompt=args.style
        )
        
        print(f"ğŸ‰ æˆåŠŸç”Ÿæˆ Sprite Sheet: {args.output}")
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 